{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Prompt Templates","text":"<p>Prompt templates have become key artifacts for researchers and practitioners working with AI. There is, however, no standardized way of sharing prompt templates. Prompts and prompt templates are shared on the Hugging Face Hub in .txt files, in HF datasets, as strings in model cards, or on GitHub as python strings embedded in scripts, in JSON and YAML files, or in Jinja2 files.</p>"},{"location":"#objectives-and-non-objectives-of-this-library","title":"Objectives and non-objectives of this library","text":""},{"location":"#objectives","title":"Objectives","text":"<ul> <li>Provide functionality for working with prompt templates locally and sharing them on the Hugging Face Hub. </li> <li>Propose a prompt template standard through .yaml and .json files that enables modular development of complex LLM systems and is interoperable with other libraries</li> </ul>"},{"location":"#non-objective","title":"Non-Objective","text":"<ul> <li>Compete with full-featured prompting libraries like LangChain, ell, etc. The objective is, instead, a simple solution for working with prompt templates locally or on the HF Hub, which is interoperable with other libraries and which the community can build upon.</li> </ul>"},{"location":"#documentation","title":"Documentation","text":"<p>A discussion of the standard prompt format, usage examples, the API reference etc. are available in the docs.</p>"},{"location":"#quick-start","title":"Quick start","text":"<p>Let's use this closed_system_prompts repo of official prompts from OpenAI and Anthropic. These prompt templates have either been leaked or were shared by these LLM providers, but were originally in a non-machine-readable, non-standardized format.</p>"},{"location":"#1-install-the-library","title":"1. Install the library:","text":"<pre><code>pip install prompt-templates\n</code></pre>"},{"location":"#2-list-available-prompts-in-a-hf-hub-repository","title":"2. List available prompts in a HF Hub repository.","text":"<pre><code>&gt;&gt;&gt; from prompt_templates import list_prompt_templates\n&gt;&gt;&gt; files = list_prompt_templates(\"MoritzLaurer/closed_system_prompts\")\n&gt;&gt;&gt; print(files)\n['claude-3-5-artifacts-leak-210624.yaml', 'claude-3-5-sonnet-text-090924.yaml', 'claude-3-5-sonnet-text-image-090924.yaml', 'openai-metaprompt-audio.yaml', 'openai-metaprompt-text.yaml']\n</code></pre>"},{"location":"#3-download-and-inspect-a-prompt-template","title":"3. Download and inspect a prompt template","text":"<pre><code>&gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n&gt;&gt;&gt; prompt_template = ChatPromptTemplate.load_from_hub(\n...     repo_id=\"MoritzLaurer/closed_system_prompts\",\n...     filename=\"claude-3-5-artifacts-leak-210624.yaml\"\n... )\n&gt;&gt;&gt; # Inspect template\n&gt;&gt;&gt; print(prompt_template.template)\n[{'role': 'system',\n  'content': '&lt;artifacts_info&gt;\\nThe assistant can create and reference artifacts ...'},\n {'role': 'user', 'content': '{{user_message}}'}]\n&gt;&gt;&gt; # Check required template variables\n&gt;&gt;&gt; print(prompt_template.template_variables)\n['current_date', 'user_message']\n&gt;&gt;&gt; print(prompt_template.metadata)\n{'source': 'https://gist.github.com/dedlim/6bf6d81f77c19e20cd40594aa09e3ecd'}\n</code></pre>"},{"location":"#4-populate-the-template-with-variables","title":"4. Populate the template with variables","text":"<p>By default, the populated prompt is returned in the OpenAI messages format, which is compatible with most open-source LLM clients.</p> <pre><code>&gt;&gt;&gt; messages = prompt_template.populate(\n...     user_message=\"Create a tic-tac-toe game for me in Python\",\n...     current_date=\"Wednesday, 11 December 2024\"\n... )\n&gt;&gt;&gt; print(messages)  # doctest: +SKIP\n[{'role': 'system', 'content': '&lt;artifacts_info&gt;\\nThe assistant can create and reference artifacts during conversations. Artifacts are ...'}, {'role': 'user', 'content': 'Create a tic-tac-toe game for me in Python'}]\n</code></pre>"},{"location":"#5-use-the-populated-template-with-any-llm-client","title":"5. Use the populated template with any LLM client","text":"<pre><code>&gt;&gt;&gt; #!pip install openai\n&gt;&gt;&gt; from openai import OpenAI\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n&gt;&gt;&gt; response = client.chat.completions.create(\n...     model=\"gpt-4o-mini\",\n...     messages=messages\n... )\n&gt;&gt;&gt; print(response.choices[0].message.content[:100], \"...\")  # doctest: +SKIP\nHere's a simple text-based Tic-Tac-Toe game in Python. This code allows two players to take turns pl ...\n</code></pre> <pre><code>&gt;&gt;&gt; from huggingface_hub import InferenceClient\n&gt;&gt;&gt; client = InferenceClient(api_key=os.environ.get(\"HF_TOKEN\"))\n&gt;&gt;&gt; response = client.chat.completions.create(\n...     model=\"meta-llama/Llama-3.3-70B-Instruct\", \n...     messages=messages,\n...     max_tokens=500\n... )\n&gt;&gt;&gt; print(response.choices[0].message.content[:100], \"...\")  # doctest: +SKIP\n&lt;antThinking&gt;Creating a tic-tac-toe game in Python is a good candidate for an artifact. It's a self- ...\n</code></pre> <p>If you use an LLM client that expects a format different to the OpenAI messages standard, you can easily reformat the prompt for this client. For example with Anthropic:</p> <pre><code>&gt;&gt;&gt; #! pip install anthropic\n&gt;&gt;&gt; from anthropic import Anthropic\n&gt;&gt;&gt; from prompt_templates import format_for_client\n\n&gt;&gt;&gt; messages_anthropic = format_for_client(messages, client=\"anthropic\")\n\n&gt;&gt;&gt; client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n&gt;&gt;&gt; response = client.messages.create(\n...     model=\"claude-3-sonnet-20240229\",\n...     system=messages_anthropic[\"system\"],\n...     messages=messages_anthropic[\"messages\"],\n...     max_tokens=1000\n... )\n&gt;&gt;&gt; print(response.content[0].text[:100], \"...\")  # doctest: +SKIP\nSure, I can create a tic-tac-toe game for you in Python. Here's a simple implementation: ...\n</code></pre> <p>Or with the Google Gen AI SDK for Gemini 2.0</p> <pre><code>&gt;&gt;&gt; #!pip install google-genai\n&gt;&gt;&gt; from google import genai\n&gt;&gt;&gt; from google.genai import types\n&gt;&gt;&gt; from prompt_templates import format_for_client\n\n&gt;&gt;&gt; messages_google = format_for_client(messages, client=\"google\")\n\n&gt;&gt;&gt; client = genai.Client(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n\n&gt;&gt;&gt; response = client.models.generate_content(\n...     model='gemini-2.0-flash-exp',\n...     contents=messages_google[\"contents\"],\n...     config=types.GenerateContentConfig(\n...         system_instruction=messages_google[\"system_instruction\"],\n...     )\n... )\n&gt;&gt;&gt; print(response.text[:100], \"...\")  # doctest: +SKIP\n</code></pre>"},{"location":"#6-create-your-own-prompt-templates","title":"6. Create your own prompt templates","text":"<pre><code>&gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n&gt;&gt;&gt; messages_template = [\n...     {\"role\": \"system\", \"content\": \"You are a coding assistant who explains concepts clearly and provides short examples.\"},\n...     {\"role\": \"user\", \"content\": \"Explain what {{concept}} is in {{programming_language}}.\"}\n... ]\n&gt;&gt;&gt; template_variables = [\"concept\", \"programming_language\"]\n&gt;&gt;&gt; metadata = {\n...     \"name\": \"Code Teacher\",\n...     \"description\": \"A simple chat prompt for explaining programming concepts with examples\",\n...     \"tags\": [\"programming\", \"education\"],\n...     \"version\": \"0.0.1\",\n...     \"author\": \"Guido van Bossum\"\n... }\n&gt;&gt;&gt; prompt_template = ChatPromptTemplate(\n...     template=messages_template,\n...     template_variables=template_variables,\n...     metadata=metadata,\n... )\n\n&gt;&gt;&gt; print(prompt_template)\nChatPromptTemplate(template=[{'role': 'system', 'content': 'You are a coding a..., template_variables=['concept', 'programming_language'], metadata={'name': 'Code Teacher', 'description': 'A simple ..., client_parameters={}, custom_data={}, populator='jinja2', jinja2_security_level='standard')\n</code></pre>"},{"location":"#7-store-or-share-your-prompt-templates","title":"7. Store or share your prompt templates","text":"<p>You can then store your prompt template locally or share it on the HF Hub.</p> <pre><code>&gt;&gt;&gt; # save locally\n&gt;&gt;&gt; prompt_template.save_to_local(\"./tests/test_data/example_prompts/code_teacher_test.yaml\")\n&gt;&gt;&gt; # or save it on the HF Hub\n&gt;&gt;&gt; prompt_template.save_to_hub(repo_id=\"MoritzLaurer/example_prompts_test\", filename=\"code_teacher_test.yaml\", create_repo=True)  # doctest: +SKIP\nCommitInfo(commit_url='https://huggingface.co/MoritzLaurer/example_prompts_test/commit/4cefd2c94f684f9bf419382f96b36692cd175e84', commit_message='Upload prompt template code_teacher_test.yaml', commit_description='', oid='4cefd2c94f684f9bf419382f96b36692cd175e84', pr_url=None, repo_url=RepoUrl('https://huggingface.co/MoritzLaurer/example_prompts_test', endpoint='https://huggingface.co', repo_type='dataset', repo_id='MoritzLaurer/example_prompts_test'), pr_revision=None, pr_num=None)\n</code></pre>"},{"location":"agents/","title":"Agents","text":"<p>Note</p> <p>These are some experimental notes on sharing agents. The new smolagents library provides a great way of sharing agents. </p> <p>How could the sharing of agents be standardized on the HF Hub?</p> <p>A good standard for sharing agents should be: modular, open, and interoperable. </p>"},{"location":"agents/#modularity-main-components-of-agents","title":"Modularity: Main components of agents","text":"<p>Agents have four main components:</p> <ol> <li>An orchestration library such as autogen, CrewAI, langchain, or transformers.agents, which implements prompt formatting, tool parsing, API calls, agent interaction etc.</li> <li>A set of prompt templates that define different tasks and agent personas.</li> <li>A set of tools, which are essentially a prompt template + code.</li> <li>A compute environment to run the agent code, invoking the prompts and tools.</li> </ol> <p>Modularity is a fundamental principle in software engineering. It enables maintainability, reusability, scalability, and testability. In practice, however, the code for LLM systems and agents often combines prompt strings, tool functions and the agent orchestration code in the same files. This means that changes in prompts are hard to test and version and it is harder for others to reuse prompt templates or tools for their own systems. </p> <p>Following the principle of modularity, agents should be shared in a directory of modular .yaml/.json files for prompt templates; .py files for tools; and a single agent.py file for the orchestration code. </p>"},{"location":"agents/#openness-sharing-and-running-agents-on-the-hf-hub","title":"Openness: Sharing and running agents on the HF Hub","text":"<p>HF Space repositories provide a suitable unit for storing the files for prompt templates (.json or .yaml files), tools (.py files) and orchestration code (single agent.py file) in a single directory, combined with attached compute for executing the agent. One Space can contain one agent, which can be executed on a free CPU, or with high-end GPUs if required. HF Spaces can be public, private or shared with a group of people in a specific organization.</p> <p>[TODO: add example of a HF Space repo with an agent.]</p> <p>Open question: How can individual prompts and tools be made easily findable and likeable for the community, if they are only files within a repository? </p>"},{"location":"agents/#interoperability","title":"Interoperability","text":"<p>Prompts and tools can be made interoperable by breaking them down into the basic file format most libraries use: prompts in .json/.yaml files following the OAI messages format and tools in .py files with functions and doc strings. Only the orchestration code in the agent.py file is non-standardized and can use the code of any orchestration framework, calling on the modular and standardized tools and prompts. </p>"},{"location":"create_template/","title":"Create a prompt template","text":"<p>You can create and save a prompt template in a few simple lines of code. </p> <ul> <li>Create a <code>ChatPromptTemplate</code> if your template is composed of multiple messages. Messages must follow the  OpenAI chat message format for standardization (they can always be converted to formats for other LLM clients in a later step via the utils). </li> <li>Create a <code>TextPromptTemplate</code> if your template is a single string and end users should insert it into the adequate message role themselves.</li> </ul>"},{"location":"create_template/#textprompttemplates","title":"TextPromptTemplates","text":"<pre><code>from prompt_templates import TextPromptTemplate\n\ntemplate = \"\"\"\\\nTranslate the following text to {{language}}:\n{{text}}\n\"\"\"\n\ntemplate_variables = [\"language\", \"text\"]  # for input validation to avoid hidden errors\nmetadata = {\n    \"name\": \"Simple Translator\",\n    \"description\": \"A simple translation prompt for illustrating the standard prompt YAML format\",\n    \"tags\": [\"translation\", \"multilinguality\"],\n    \"version\": \"0.0.1\",\n    \"author\": \"Guy van Babel\",\n}\nclient_parameters = {\"temperature\": 0}\n\nprompt_template = TextPromptTemplate(\n    template=template,\n    template_variables=template_variables,\n    metadata=metadata,\n    client_parameters=client_parameters,\n)\n</code></pre>"},{"location":"create_template/#chatprompttemplates","title":"ChatPromptTemplates","text":"<pre><code>from prompt_templates import ChatPromptTemplate\n\ntemplate_messages = [\n    {\"role\": \"system\", \"content\": \"You are a coding assistant who explains concepts clearly and provides short examples.\"},\n    {\"role\": \"user\", \"content\": \"Explain what {{concept}} is in {{programming_language}}.\"},\n]\ntemplate_variables = [\"concept\", \"programming_language\"]  # for input validation to avoid hidden errors\nmetadata = {\n    \"name\": \"Code Teacher\",\n    \"description\": \"A simple chat prompt for explaining programming concepts with examples\",\n    \"tags\": [\"programming\", \"education\"],\n    \"version\": \"0.0.1\",\n    \"author\": \"Guido van Bossum\",\n}\nclient_parameters = {\"temperature\": 0}\n\nprompt_template = ChatPromptTemplate(\n    template=template_messages,\n    template_variables=template_variables,\n    metadata=metadata,\n    client_parameters=client_parameters,\n)\n</code></pre>"},{"location":"create_template/#save-the-prompt-template-in-a-separate-file","title":"Save the prompt template in a separate file","text":"<pre><code># save template locally or on the HF Hub\nfilename = \"code_teacher.yaml\"\nprompt_template.save_to_local(f\"./example_prompts/{filename}\")\nprompt_template.save_to_hub(\n    repo_id=\"MoritzLaurer/example_prompts\", \n    filename=filename, \n    create_repo=True,\n)\n</code></pre>"},{"location":"create_template/#when-not-to-create-modular-prompt-template-files","title":"When NOT to create modular prompt template files","text":"<p>When you just started testing prompts for a new task it might be overkill to create prompt templates in separate files. In the earily stage of testing you can keep keep your prompts embedded in your scripts for simplicity. </p>"},{"location":"create_template/#when-to-create-modular-prompt-template-files","title":"When to create modular prompt template files","text":"<p>After initial vibe-tests with a prompt template, it is good practice to test the template more systematically. When you reach this stage of more systematic testing, you should start storing and versioning your prompt templates in separate .yaml files.</p> <p>Disentangling your code from your prompt templates enables you to:</p> <ul> <li>compare which formulation of your template works best on test data</li> <li>invite colleagues to try and create their own versions and compare them</li> </ul> <p>Think of your prompt templates as the key hyperparameters of your LLM system, where the prompt formulation on the client parameters are the key factors that determine the performance of the system. </p> <p>Making prompt templates modular makes them systematically testable, sharable and reusable. </p>"},{"location":"repo_types_examples/","title":"Prompt templates on the HF Hub","text":"<p>The HF Hub is currently organized around three main repository types:</p> <ul> <li>Model repositories: Repos with model weights, tokenizers, and model configs.</li> <li>Dataset repositories: Repos with tabular datasets (mostly in parquet format). </li> <li>Spaces repositories: Repos with hosted applications (often with code and data, which is then visualized in the Space).</li> </ul> <p>Prompt templates can be saved into any of these repository types as .yaml or .json files. We recommend saving prompt templates in dataset repos by default. </p>"},{"location":"repo_types_examples/#1-saving-collections-of-prompt-templates-as-independent-artifacts-in-dataset-repos","title":"1. Saving collections of prompt templates as independent artifacts in dataset repos","text":"<p>Many prompt templates can be reused in different projects and with different models. We recommend sharing collections of reusable prompt templates in HF dataset repos, where the dataset card provides a description and usage instructions and the templates are shared as .yaml or .json files in the same repository.</p> 1. Example: using the leaked Claude Artifacts prompt  #### List all prompt templates stored in a HF dataset repo This [example HF repository](https://huggingface.co/MoritzLaurer/closed_system_prompts)  contains leaked or released prompts from Anthropic and OpenAI.   <pre><code>from prompt_templates import list_prompt_templates\nlist_prompt_templates(repo_id=\"MoritzLaurer/closed_system_prompts\")\n# ['claude-3-5-artifacts-leak-210624.yaml', 'claude-3-5-sonnet-text-090924.yaml', 'claude-3-5-sonnet-text-image-090924.yaml', 'openai-metaprompt-audio.yaml', 'openai-metaprompt-text.yaml']\n</code></pre>  #### Download a specific prompt template Here, we download the leaked prompt for Claude-3.5 Sonnet for creating Artifacts.   <pre><code>from prompt_templates import ChatPromptTemplate\nprompt_template = ChatPromptTemplate.load_from_hub(\n    repo_id=\"MoritzLaurer/closed_system_prompts\",\n    filename=\"claude-3-5-artifacts-leak-210624.yaml\"\n)\n\nprint(prompt_template)\n# ChatPromptTemplate(template=[{'role': 'system', 'content': '&lt;artifacts_info&gt; The assistant can create and reference artifacts during conversations. Artifacts are ... Claude is now being connected with a human.'}, {'role': 'user', 'content': '{user_message}'}], template_variables=['current_date', 'user_message'], metadata=[{'source': 'https://gist.github.com/dedlim/6bf6d81f77c19e20cd40594aa09e3ecd'}])\n</code></pre>  Prompt templates are downloaded as either `ChatPromptTemplate` or `TextPromptTemplate` classes. This class makes it easy to populate a prompt template and convert it into a format that's compatible with different LLM clients. The type is automatically determined based on whether the YAML contains a simple string (TextPromptTemplate) or a list of dictionaries following the OpenAI messages format (ChatPromptTemplate).  #### Populate and use the prompt template We can then populate the prompt template for a specific use-case.  <pre><code># Check which variables the prompt template requires\nprint(prompt_template.template_variables)\n# ['current_date', 'user_message']\n\nuser_message = \"Create a simple calculator web application\"\nmessages = prompt_template.populate(\n    user_message=user_message, \n    current_date=\"Monday 21st October 2024\", \n)\n\n# The default output is in the OpenAI messages format. We can easily reformat it for another client.\nfrom prompt_templates import format_for_client\n\nmessages_anthropic = format_for_client(messages, client=\"anthropic\")\n</code></pre>  The output is a list or a dictionary in the format expected by the specified LLM client. For example, OpenAI expects a list of message dictionaries, while Anthropic expects a dictionary with \"system\" and \"messages\" keys.  <pre><code>#!pip install anthropic\nfrom anthropic import Anthropic\nclient_anthropic = Anthropic()\n\nresponse = client_anthropic.messages.create(\n    model=\"claude-3-5-sonnet-20240620\",\n    system=messages_anthropic[\"system\"],\n    messages=messages_anthropic[\"messages\"],\n    max_tokens=4096,\n)\n</code></pre> 2. Example: JudgeBench paper prompts The paper \"JudgeBench: A Benchmark for Evaluating LLM-Based Judges\" (paper) collects several prompts for using LLMs to evaluate unstructured LLM outputs. After copying them into a HF Hub dataset repo in the standardized YAML format, they can be directly loaded and populated.  <pre><code>from prompt_templates import ChatPromptTemplate\nprompt_template = ChatPromptTemplate.load_from_hub(\n  repo_id=\"MoritzLaurer/prompts_from_papers\", \n  filename=\"judgebench-vanilla-prompt.yaml\"\n)\n</code></pre> 3. Example: Sharing closed system prompts The community has extracted system prompts from closed API providers like OpenAI or Anthropic and these prompts are unsystematically shared via GitHub, Reddit etc. (e.g. Anthropic Artifacts prompt). Some API providers have also started sharing their system prompts on their websites in non-standardized HTML (Anthropic, OpenAI). To simplify to use of these prompts, they can be shared in a HF Hub dataset repo as standardized YAML files.     <pre><code>from prompt_templates import list_prompt_templates, ChatPromptTemplate\nlist_prompt_templates(repo_id=\"MoritzLaurer/closed_system_prompts\")\n# out: ['claude-3-5-artifacts-leak-210624.yaml', 'claude-3-5-sonnet-text-090924.yaml', 'claude-3-5-sonnet-text-image-090924.yaml', 'openai-metaprompt-audio.yaml', 'openai-metaprompt-text.yaml']\n\nprompt_template = ChatPromptTemplate.load_from_hub(\n  repo_id=\"MoritzLaurer/closed_system_prompts\", \n  filename=\"openai-metaprompt-text.yaml\"\n)\n</code></pre>"},{"location":"repo_types_examples/#2-attaching-prompt-templates-to-models","title":"2. Attaching prompt templates to models","text":"<p>Some open-weight LLMs have been trained to exhibit specific behaviours with specific prompt templates. The vision language model InternVL2 was trained to predict bounding boxes for manually specified areas with a special prompt template;  the VLM Molmo was trained to predict point coordinates of objects of images with a special prompt template; etc.</p> <p>These prompt templates are currently either mentioned unsystematically in model cards or need to be tracked down on github or paper appendices by users. </p> <p><code>prompt_templates</code> proposes to share these types of prompt templates in YAML or JSON files in the model repository together with the model weights. </p> 1. Example: Sharing the InternVL2 special task prompt templates <pre><code># download image prompt template\nfrom prompt_templates import ChatPromptTemplate\nprompt_template = ChatPromptTemplate.load_from_hub(\n  repo_id=\"MoritzLaurer/open_models_special_prompts\", \n  filename=\"internvl2-bbox-prompt.yaml\"\n)\n\n# populate prompt\nimage_url = \"https://unsplash.com/photos/ZVw3HmHRhv0/download?ixid=M3wxMjA3fDB8MXxhbGx8NHx8fHx8fDJ8fDE3MjQ1NjAzNjl8&amp;force=true&amp;w=1920\"\nregion_to_detect = \"the bird\"\nmessages = prompt_template.populate(image_url=image_url, region_to_detect=region_to_detect)\n\nprint(messages)\n#[{'role': 'user',\n#  'content': [{'type': 'image_url',\n#    'image_url': {'url': 'https://unsplash.com/photos/ZVw3HmHRhv0/download?ixid=M3wxMjA3fDB8MXxhbGx8NHx8fHx8fDJ8fDE3MjQ1NjAzNjl8&amp;force=true&amp;w=1920'}},\n#   {'type': 'text',\n#    'text': 'Please provide the bounding box coordinate of the region this sentence describes: &lt;ref&gt;the bird&lt;/ref&gt;'}]}]\n</code></pre>  This populated prompt can then directly be used in a vLLM container, e.g. hosted on HF Inference Endpoints, using the OpenAI messages format and client.  <pre><code>from openai import OpenAI\nimport os\n\nENDPOINT_URL = \"https://tkuaxiztuv9pl4po.us-east-1.aws.endpoints.huggingface.cloud\" + \"/v1/\" \n\n# initialize the OpenAI client but point it to an endpoint running vLLM or TGI\nclient = OpenAI(\n    base_url=ENDPOINT_URL, \n    api_key=os.getenv(\"HF_TOKEN\")\n)\n\nresponse = client.chat.completions.create(\n    model=\"/repository\", # with vLLM deployed on HF endpoint, this needs to be /repository since there are the model artifacts stored\n    messages=messages,\n)\n\nresponse.choices[0].message.content\n# out: 'the bird[[54, 402, 515, 933]]'\n</code></pre>"},{"location":"repo_types_examples/#3-attaching-prompt-templates-to-datasets","title":"3. Attaching prompt templates to datasets","text":"<p>LLMs are increasingly used to help create datasets, for example for quality filtering or synthetic text generation. The prompt templates used for creating a dataset are currently unsystematically shared on GitHub (example),  referenced in dataset cards (example), or stored in .txt files (example),  hidden in paper appendices or not shared at all.  This makes reproducibility unnecessarily difficult.</p> <p>To facilitate reproduction, these dataset prompt templates can be shared in YAML files in HF dataset repositories together with metadata on generation parameters, model_ids etc. </p> 1. Example: the FineWeb-edu prompt The FineWeb-Edu dataset was created by prompting `Meta-Llama-3-70B-Instruct` to score the educational value of web texts. The authors provide the prompt template in a .txt file.  When provided in a YAML/JSON file in the dataset repo, the prompt template can easily be loaded and supplemented with client_parameters like the model_id or generation parameters for easy reproducibility.  See this example dataset repository <pre><code>from prompt_templates import ChatPromptTemplate\nimport torch\nfrom transformers import pipeline\n\nprompt_template = ChatPromptTemplate.load_from_hub(\n  repo_id=\"MoritzLaurer/dataset_prompts\", \n  filename=\"fineweb-edu-prompt.yaml\", \n)\n\n# populate the prompt\ntext_to_score = \"The quick brown fox jumps over the lazy dog\"\nmessages = prompt_template.populate(text_to_score=text_to_score)\n\n# test prompt with local llama\nmodel_id = \"meta-llama/Llama-3.2-1B-Instruct\"  # prompt was original created for meta-llama/Meta-Llama-3-70B-Instruct\n\npipe = pipeline(\n    \"text-generation\",\n    model=model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\n\noutputs = pipe(\n    messages,\n    max_new_tokens=512,\n)\n\nprint(outputs[0][\"generated_text\"][-1])\n</code></pre> 2. Example: the Cosmopedia dataset Cosmopedia is a dataset of synthetic textbooks, blogposts, stories, posts and WikiHow articles generated by Mixtral-8x7B-Instruct-v0.1. The dataset shares it's prompt templates on GitHub with a custom build logic. The prompts are not available in the HF dataset repo  The prompts could be directly added to the dataset repository in the standardized YAML/JSON format."},{"location":"repo_types_examples/#4-attaching-prompt-templates-to-hf-spaces","title":"4. Attaching prompt templates to HF Spaces","text":"<p>[TODO: create example]</p>"},{"location":"standard_prompt_format/","title":"Standardizing prompt templates","text":"<p>The library expects prompt templates to be stored as modular YAML or JSON files. They can be stored locally or in a HF repository.</p> <p>A prompt template YAML or JSON file must follow the following standardized structure:</p> <ul> <li>Top-level key (required): <code>prompt</code>. This top-level key signals to the parser that the content of the file is a prompt template.</li> <li>Second-level key (required): <code>template</code>. This can be either a simple string, or a list of dictionaries following the OpenAI messages format. The messages format is recommended for use with LLM APIs or inference containers. Variable placeholders for populating the prompt template string are denoted with double curly brackets {{...}}.</li> <li>Second-level keys (optional): (1) <code>template_variables</code> (list): variables for populating the prompt template. This is used for input validation and to make the required variables for long templates easily accessible; (2) <code>metadata</code> (dict): information about the template such as the source, date, author etc.; (3) <code>client_parameters</code> (dict): parameters for the inference client (e.g. temperature, model_id); (4) <code>custom_data</code> (dict): any other data that does not fit into the other categories.</li> </ul> <p>Example prompt template following the standard in YAML:  <pre><code>prompt:\n  template:\n    - role: \"system\"\n      content: \"You are a coding assistant who explains concepts clearly and provides short examples.\"\n    - role: \"user\"\n      content: \"Explain what {{concept}} is in {{programming_language}}.\"\n  template_variables:\n    - concept\n    - programming_language\n  metadata:\n    name: \"Code Teacher\"\n    description: \"A simple chat prompt for explaining programming concepts with examples\"\n    tags:\n      - programming\n      - education\n    version: \"0.0.1\"\n    author: \"Guido van Bossum\"\n  client_parameters:\n    - temperature: 0\n</code></pre></p> <p>Repository types on the HF Hub: Prompt template files can be shared in any HF repo type (dataset/model/space repo). We recommend sharing collections of prompt templates in dataset repos by default. See details here.</p> <p>Naming convention: We call a file a \"prompt template\", when it has placeholders ({{...}}) for dynamically populating the template similar to an f-string. This makes files more useful and reusable by others for different use-cases. Once the placeholders in the template are populated with specific variables, we call it a \"prompt\". </p> <p>Templating: Jinja2 is the default templating engine for populating the variables in the template. </p> <p>The following example illustrates how the prompt template becomes a prompt. </p> <pre><code>&gt;&gt;&gt; # 1. Download a prompt template:\n&gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n&gt;&gt;&gt; prompt_template = ChatPromptTemplate.load_from_hub(\n...     repo_id=\"MoritzLaurer/example_prompts\",\n...     filename=\"code_teacher.yaml\"\n... )\n\n&gt;&gt;&gt; # 2. Inspect the template and it's variables:\n&gt;&gt;&gt; prompt_template.template\n[{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what {{concept}} is in {{programming_language}}.'}]\n&gt;&gt;&gt; prompt_template.template_variables\n['concept', 'programming_language']\n\n&gt;&gt;&gt; # 3. Populate the template with its variables\n&gt;&gt;&gt; prompt = prompt_template.populate(\n...     concept=\"list comprehension\",\n...     programming_language=\"Python\"\n... )\n&gt;&gt;&gt; print(prompt)\n[{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]\n</code></pre>"},{"location":"standard_prompt_format/#proscons-for-different-file-formats-for-sharing-prompt-templates","title":"Pros/Cons for different file formats for sharing prompt templates","text":""},{"location":"standard_prompt_format/#procon-prompts-as-yaml-files","title":"Pro/Con prompts as YAML files","text":"<ul> <li>Existing prompt hubs use YAML (or JSON): LangChain Hub (see also this);  Haystack Prompt Hub</li> <li>YAML (or JSON) is the standard for working with prompts in production settings in my experience with practitioners. See also this discussion.</li> <li>Managing individual prompt templates in separate YAML files makes each prompt template an independent modular unit. <ul> <li>This makes it e.g. easier to add metadata and production-relevant information in the respective prompt YAML file.</li> <li>Prompt templates in individual YAML files also enables users to add individual prompts into any HF repo abstraction (Dataset, Model, Space repos), while tabular dataset file types are only compatible with one specific repo type.</li> </ul> </li> </ul>"},{"location":"standard_prompt_format/#procon-json-files","title":"Pro/Con JSON files","text":"<ul> <li>The same pro arguments of YAML also apply to JSON. </li> <li>Directly parsable as Python dict, similar to YAML</li> <li>More verbose to type and less pretty than YAML, but probably more familiar to some users</li> </ul>"},{"location":"standard_prompt_format/#procon-jinja2-files","title":"Pro/Con Jinja2 files","text":"<ul> <li>Has more rich functionality for populating prompt templates</li> <li>Can be directly integrated into YAML or JSON, so can always be added to the common YAML/JSON standard</li> <li>Issue: allows arbitrary code execution and is less safe</li> <li>Harder to read for beginners</li> </ul>"},{"location":"standard_prompt_format/#procon-tabular-file-formats-eg-parquet","title":"Pro/Con tabular file formats (e.g. parquet)","text":"<ul> <li>Some tabular prompt datasets like awesome-chatgpt-prompts have received many likes on HF</li> <li>The dataset viewer allows for easy and quick visualization</li> <li>Main cons: the tabular data format is not well suited for reusing prompt templates  and is not standard among practitioners<ul> <li>Extracting a single prompt from a tabular dataset with dataset/pandas-like operations is unnecessarily complicated.</li> <li>In industry practice, prompt templates are independent modular units that can be reused for different use-cases. Having multiple templates in the same dataset forces different templates to have the same column structure and prevents proper modular development.  </li> <li>Datasets on the HF hub are in parquet files, which are not easily editable. Editing a prompt in JSON or YAML is much easier than editing a (parquet) dataset. </li> <li>Data viewers for tabular data are bad for visualizing the structure of long prompt templates (where e.g. line breaks have an important substantive meaning). Viewing and editing prompt templates in markdown-like editors is more standard in the ecosystem.</li> <li>Saving prompt templates as datasets prevents them from being modular components of model or space repos (see example use-cases for this) </li> </ul> </li> </ul>"},{"location":"standard_prompt_format/#compatibility-with-langchain","title":"Compatibility with LangChain","text":"<p>LangChain is a great library for creating interoperability between different LLM clients. This library is inspired by LangChain's PromptTemplate and ChatPromptTemplate classes. One difference is that the LangChain ChatPromptTemplate expects a \"messages\" key instead of a \"template\" key for the prompt template in the messages format. This HF library uses the \"template\" key both for HF TextPromptTemplate and for HF ChatPromptTemplate for simplicity. If you still load a YAML/JSON file with a \"messages\" key, it will be automatically renamed to \"template\". You can also always convert a HF PromptTemplate to a LangChain template with .to_langchain_template(). The objective of this library is not to reproduce the full functionality of a library like LangChain, but to enable the community to share prompts on the HF Hub and load and reuse them with any of their favourite libraries. </p> <p>A <code>PromptTemplate</code> from <code>prompt-templates</code> can be easily converted to a langchain template: </p> <pre><code>from prompt_templates import ChatPromptTemplate\nprompt_template = ChatPromptTemplate.load_from_hub(\n    repo_id=\"MoritzLaurer/example_prompts\",\n    filename=\"code_teacher.yaml\"\n)\nprompt_template_langchain = prompt_template.to_langchain_template()\n</code></pre>"},{"location":"standard_prompt_format/#notes-on-compatibility-with-transformers","title":"Notes on compatibility with <code>transformers</code>","text":"<ul> <li><code>transformers</code> provides partial prompt input standardization via chat_templates following the OpenAI messages format:<ul> <li>The simplest use is via the text-generation pipeline</li> <li>See also details on chat_templates.</li> </ul> </li> <li>Limitations: <ul> <li>The original purpose of these chat_templates is to easily add special tokens that a specific open-source model requires under the hood. The <code>prompt_templates</code> library is designed for prompt templates for any LLM, not just open-source LLMs.   </li> <li>VLMs require special pre-processors that are not directly compatible with the standardized messages format (?). And new VLMs like InternVL or Molmo often require non-standardized remote code for image preprocessing. </li> <li>LLMs like command-r have cool special prompts e.g. for grounded generation, but they provide their own custom remote code for preparing prompts/functionalities properly for these special prompts.</li> </ul> </li> </ul>"},{"location":"standard_prompt_format/#existing-prompt-template-repos","title":"Existing prompt template repos","text":"<ul> <li>LangChain Hub for prompts (main hub is proprietary. See the old public oss repo, using JSON or YAML, with {...} for template variables)</li> <li>LangGraph Templates (underlying data structure unclear, does not seem to have a collaborative way of sharing templates)</li> <li>LlamaHub (seems to use GitHub as backend)</li> <li>Deepset Prompt Hub (seems not maintained anymore, used YAML with {...} for template variables)</li> <li>distilabel templates and tasks (source) (using pure jinja2 with {{ ... }} for template variables)</li> <li>Langfuse, see also example here (no public prompt repo, using JSON internally with {{...}} for template variables)</li> <li>Promptify (not maintained anymore, used jinja1 and {{ ... }} for template variables)</li> </ul>"},{"location":"standard_tool_format/","title":"Standardizing and Sharing Tools","text":"<p>Note</p> <p>These are some experimental notes on sharing tools. The new smolagents library provides a great way of sharing tools. </p>"},{"location":"standard_tool_format/#what-are-llm-tools","title":"What are LLM tools?","text":"<p>Imagine you want to build a financial chatbot. For a good chatbot, it is not enough to just generate convincing text, you might also want it to be able to fetch recent financial information or do calculations. While LLM can only generate text, their text output can be used as input to external code, which does some useful action. This external code is called a \"function\" or a \"tool\".</p> <p>Different companies use slightly different language and implementations for this same idea: OpenAI uses the term \"function calling\" when text input for a single function is produced and the term \"tool use\" when an LLM assistant has some autonomy to produce text input for one out of several functions; Anthropic primarily uses the term \"tool use\" (and function calling as a synonym); similar to Mistral; similar to open-source inference engines like TGI or vLLM, which have converged on OpenAI's API specification. (Note that these APIs all follow the JsonAgent paradigm, which is slightly different to the CodeAgent paradigm)</p>"},{"location":"standard_tool_format/#main-components-of-tools","title":"Main components of tools","text":"<p>LLM tools have the following main components: </p> <ol> <li>A textual description of the tool, including its inputs and outputs. This description is passed to the LLM's prompt, to enable it to produce text outputs that fit to the tool's description. For closed-source LLM, this integration of the tool description into the prompt is hidden. </li> <li>Code that implements the tool. For example a simple Python function taking a search query text as input, does an API call, and returns ranked search results as output. </li> <li>A compute environment in which the tool's code is executed. This can e.g. be your local computers' development environment, or docker container running on a cloud CPU. </li> </ol>"},{"location":"standard_tool_format/#current-formats-for-sharing-tools","title":"Current formats for sharing tools","text":"<p>The tutorials of LLM API providers format tools either as Python dictionaries or JSON strings (OpenAI, Anthropic, Mistral, TGI, vLLM), which are integrated into example scripts.</p> <p>LLM agent libraries all have their own implementations of tools for their library: LangChain Tools, LangChain Community Tools or Agent Toolkits (docs); LlamaHub (docs, docs); CrewAI Tools (docs, including wrapper for using LangChain and LlamaHub tools); AutoGen (docs, including a LangChain tool wrapper); Transformers Agents etc.</p> <p>As all of these libraries and their tool collections are hosten on GitHub, GitHub has indirectly become the main platform for sharing LLM tools today, although it has not been designed for this purpose. </p> <p>The main standardizing force for LLM tools are the API specifications and the expected JSON input format of LLM API providers. As OpenAI is the main standard setter, most libraries are compatible with the JSON input format specified in the OpenAI function/tool calling guide and docs. In the field of agents, this has lead to the json agent paradigm. (Note that this requirement of LLM API compatibility is unnecessary in the code agent paradigm, where the LLM writes executable code itself, instead of only writing the structured input for existing code.)</p>"},{"location":"standard_tool_format/#reflections-on-the-best-formats-for-standardizing-tools","title":"Reflections on the best formats for standardizing tools","text":"<p>The most elegant and universal way of creating a tool is probably a .py file with a function and a doc string (used e.g. by CrewAI, AutoGen, LangChain and Transformers Agents). This combines the executable function code with the textual description of the tool via the doc string in an standardized way. </p> <p>For JsonAgents, the function's docstring can be parsed to construct the expected input for the LLM API and the API then resturns the required inputs for the .py file. For CodeAgents, the function can directly be passed to the LLM's prompt and the .py file is directly executable.  </p> <p>Alternatively, tools could be shared as .json files, but this would decouple the tool's description (in the .json file) from its code (e.g. in a .py file)</p>"},{"location":"template_dictionaries/","title":"PromptTemplateDictionaries","text":"<p>Note</p> <p>This feature is highly experimental and will change in the coming days.</p> <p>Complex LLM systems often depend on multiple interdependent prompt templates instead of a single template. A good example for this are agents, where the general system logic, planning steps and different tasks are defined in separate templates. It can be easier to define, read and change the interdependent templates in a single file, as opposed to separate files.</p> <p>The <code>PromptTemplateDictionary</code> is designed for these use-cases. A <code>PromptTemplateDictionary</code> is simply a dictionary of <code>ChatPromptTemplate</code>s or <code>TextPromptTemplate</code>s which are loaded as a single Python object and stored in a single YAML file. You can load and use them like this:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import PromptTemplateDictionary\n\n&gt;&gt;&gt; template_dictionary = PromptTemplateDictionary.load_from_local(\n...     file_path=\"./tests/test_data/example_prompts/agent_example_1.yaml\"\n... )\n\n&gt;&gt;&gt; print(template_dictionary.template_dictionary)  # TODO: rename attribute\n# {'agent_system_prompt': ChatPromptTemplate(template=[{'role': 'system', 'content': 'You are a code age...', template_variables=['tool_descriptions', 'task'], metadata={}, client_parameters={}, custom_data={}, populator='jinja2', jinja2_security_level='standard'),\n#  'agent_planning_prompt': TextPromptTemplate(template='Here is your task:\\n\\nTask:\\n```\\n{{task}}\\n```\\n...', template_variables=['task', 'tool_descriptions', 'managed_agents_desc...', metadata={}, client_parameters={}, custom_data={}, populator='jinja2', jinja2_security_level='standard')}\n</code></pre> <p>When integrating the <code>PromptTemplateDictionary</code> into your agent code, you can access and populate the respective template as follows. Once populated, the template becomes a list of message dicts (for <code>ChatPromptTemplate</code>) or a single string (for <code>TextPromptTemplate</code>) which can be directly passed to an LLM client.</p> <pre><code>agent_system_prompt = template_dictionary[\"agent_system_prompt\"].populate(\n    tool_descriptions=\"... some tool descriptions ...\",\n    task=\"... some task ...\",\n)\nprint(agent_system_prompt)\n# [{'role': 'system',\n#   'content': 'You are a code agent and you have the following tools at your disposal:\\n&lt;tools&gt;\\n... some tool descriptions ...\\n&lt;/tools&gt;'},\n#  {'role': 'user',\n#   'content': 'Here is the task:\\n&lt;task&gt;\\n... some task ...\\n&lt;/task&gt;\\nNow begin!'}]\n</code></pre> <p>A <code>PromptTemplateDictionary</code> is defined like this in a yaml file:</p> <pre><code>prompt:\n  template_dictionary:\n    agent_system_prompt:\n      template:\n        - role: \"system\"\n          content: |-\n            You are a code agent and you have the following tools at your disposal:\n            &lt;tools&gt;\n            {{tool_descriptions}}\n            &lt;/tools&gt;\n        - role: \"user\"\n          content: |-\n            Here is the task:\n            &lt;task&gt;\n            {{task}}\n            &lt;/task&gt;\n            Now begin!\n      template_variables:\n        - tool_descriptions\n        - task\n    agent_planning_prompt:\n      template: |-\n        Here is your task:\n\n        Task:\n        &lt;/task&gt;\n        {{task}}\n        &lt;task&gt;\n\n        Your plan can leverage any of these tools:\n        {{tool_descriptions}}\n\n        {{managed_agents_descriptions}}\n\n        List of facts that you know:\n        &lt;facts&gt;\n        {{answer_facts}}\n        &lt;/facts&gt;\n\n        Now begin! Write your plan below.\n      template_variables:\n        - task\n        - tool_descriptions\n        - managed_agents_descriptions\n        - answer_facts\n  metadata:\n    name: \"Example Code Agent\"\n    description: \"A simple code agent example\"\n    tags:\n      - agent\n    version: \"0.0.1\"\n    author: \"Guido van Bossum\"\n  client_parameters: {}\n  custom_data: {}\n</code></pre> <p>You can either create and edit these templates directly in YAML.</p> <p>Alternatively, you can create a <code>PromptTemplateDictionary</code> programmatically like this: </p> <pre><code>from prompt_templates import PromptTemplateDictionary, ChatPromptTemplate, TextPromptTemplate\n\nagent_system_prompt_template = ChatPromptTemplate(\n    template=[\n        {'role': 'system', 'content': 'You are a code agent and you have the following tools at your disposal:\\n&lt;tools&gt;\\n{{tool_descriptions}}\\n&lt;/tools&gt;'},\n        {'role': 'user', 'content': 'Here is the task:\\n&lt;task&gt;\\n{{task}}\\n&lt;/task&gt;\\nNow begin!'},\n    ],\n    template_variables=['tool_descriptions', 'task'],\n)\n\nagent_planning_prompt_template = TextPromptTemplate(\n    template='Here is your task:\\n\\nTask:\\n```\\n{{task}}\\n```\\n\\nYour plan can leverage any of these tools:\\n{{tool_descriptions}}\\n\\n{{managed_agents_descriptions}}\\n\\nList of facts that you know:\\n```\\n{{answer_facts}}\\n```\\n\\nNow begin! Write your plan below.',\n    template_variables=['task', 'tool_descriptions', 'managed_agents_descriptions', 'answer_facts'],\n)\n\ntemplate_dictionary = PromptTemplateDictionary(\n    template_dictionary={\n        \"agent_system_prompt\": agent_system_prompt_template,\n        \"agent_planning_prompt\": agent_planning_prompt_template,\n    }\n)\n\n# not implemented yet\ntemplate_dictionary.save_to_local(file_path=\"./tests/test_data/example_prompts/agent_example_test.yaml\")\ntemplate_dictionary.save_to_hub(repo_id=\"moritzlaurer/agent_example_test\", filename=\"agent_example_test.yaml\", create_repo=True)\n</code></pre>"},{"location":"reference/populators/","title":"Populating Templates","text":"<p>This section documents the classes and functions for populating templates with user-provided variables.</p>"},{"location":"reference/populators/#prompt_templates.populators","title":"prompt_templates.populators","text":""},{"location":"reference/populators/#prompt_templates.populators.TemplatePopulator","title":"TemplatePopulator","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for template populating strategies.</p> Source code in <code>prompt_templates/populators.py</code> <pre><code>class TemplatePopulator(ABC):\n    \"\"\"Abstract base class for template populating strategies.\"\"\"\n\n    @abstractmethod\n    def populate(self, template_str: str, user_provided_variables: Dict[str, Any]) -&gt; str:\n        \"\"\"Populate the template with given user_provided_variables.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_variable_names(self, template_str: str) -&gt; Set[str]:\n        \"\"\"Extract variable names from template.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/populators/#prompt_templates.populators.TemplatePopulator.populate","title":"populate  <code>abstractmethod</code>","text":"<pre><code>populate(template_str, user_provided_variables)\n</code></pre> <p>Populate the template with given user_provided_variables.</p> Source code in <code>prompt_templates/populators.py</code> <pre><code>@abstractmethod\ndef populate(self, template_str: str, user_provided_variables: Dict[str, Any]) -&gt; str:\n    \"\"\"Populate the template with given user_provided_variables.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/populators/#prompt_templates.populators.TemplatePopulator.get_variable_names","title":"get_variable_names  <code>abstractmethod</code>","text":"<pre><code>get_variable_names(template_str)\n</code></pre> <p>Extract variable names from template.</p> Source code in <code>prompt_templates/populators.py</code> <pre><code>@abstractmethod\ndef get_variable_names(self, template_str: str) -&gt; Set[str]:\n    \"\"\"Extract variable names from template.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/populators/#prompt_templates.populators.SingleBracePopulator","title":"SingleBracePopulator","text":"<p>             Bases: <code>TemplatePopulator</code></p> <p>Template populator using regex for basic {var} substitution.</p> Source code in <code>prompt_templates/populators.py</code> <pre><code>class SingleBracePopulator(TemplatePopulator):\n    \"\"\"Template populator using regex for basic {var} substitution.\"\"\"\n\n    def populate(self, template_str: str, user_provided_variables: Dict[str, Any]) -&gt; str:\n        pattern = re.compile(r\"\\{([^{}]+)\\}\")\n\n        def replacer(match: Match[str]) -&gt; str:\n            key = match.group(1).strip()\n            if key not in user_provided_variables:\n                raise ValueError(f\"Variable '{key}' not found in provided variables\")\n            return str(user_provided_variables[key])\n\n        return pattern.sub(replacer, template_str)\n\n    def get_variable_names(self, template_str: str) -&gt; Set[str]:\n        pattern = re.compile(r\"\\{([^{}]+)\\}\")\n        return {match.group(1).strip() for match in pattern.finditer(template_str)}\n</code></pre>"},{"location":"reference/populators/#prompt_templates.populators.DoubleBracePopulator","title":"DoubleBracePopulator","text":"<p>             Bases: <code>TemplatePopulator</code></p> <p>Template populator using regex for {{var}} substitution.</p> Source code in <code>prompt_templates/populators.py</code> <pre><code>class DoubleBracePopulator(TemplatePopulator):\n    \"\"\"Template populator using regex for {{var}} substitution.\"\"\"\n\n    def populate(self, template_str: str, user_provided_variables: Dict[str, Any]) -&gt; str:\n        pattern = re.compile(r\"\\{\\{([^{}]+)\\}\\}\")\n\n        def replacer(match: Match[str]) -&gt; str:\n            key = match.group(1).strip()\n            if key not in user_provided_variables:\n                raise ValueError(f\"Variable '{key}' not found in provided variables\")\n            return str(user_provided_variables[key])\n\n        return pattern.sub(replacer, template_str)\n\n    def get_variable_names(self, template_str: str) -&gt; Set[str]:\n        pattern = re.compile(r\"\\{\\{([^{}]+)\\}\\}\")\n        return {match.group(1).strip() for match in pattern.finditer(template_str)}\n</code></pre>"},{"location":"reference/populators/#prompt_templates.populators.Jinja2TemplatePopulator","title":"Jinja2TemplatePopulator","text":"<p>             Bases: <code>TemplatePopulator</code></p> <p>Jinja2 template populator with configurable security levels.</p> Security Levels <ul> <li>strict: Minimal set of features, highest security     Filters: lower, upper, title, safe     Tests: defined, undefined, none     Env: autoescape=True, no caching, no globals, no auto-reload</li> <li>standard (default): Balanced set of features     Filters: lower, upper, title, capitalize, trim, strip, replace, safe,             int, float, join, split, length     Tests: defined, undefined, none, number, string, sequence     Env: autoescape=True, limited caching, basic globals, no auto-reload</li> <li>relaxed: Default Jinja2 behavior (use with trusted templates only)     All default Jinja2 features enabled     Env: autoescape=False, full caching, all globals, auto-reload allowed</li> </ul> <p>Parameters:</p> Name Type Description Default <code>security_level</code> <code>Jinja2SecurityLevel</code> <p>Level of security restrictions (\"strict\", \"standard\", \"relaxed\")</p> <code>'standard'</code> Source code in <code>prompt_templates/populators.py</code> <pre><code>class Jinja2TemplatePopulator(TemplatePopulator):\n    \"\"\"Jinja2 template populator with configurable security levels.\n\n    Security Levels:\n        - strict: Minimal set of features, highest security\n            Filters: lower, upper, title, safe\n            Tests: defined, undefined, none\n            Env: autoescape=True, no caching, no globals, no auto-reload\n        - standard (default): Balanced set of features\n            Filters: lower, upper, title, capitalize, trim, strip, replace, safe,\n                    int, float, join, split, length\n            Tests: defined, undefined, none, number, string, sequence\n            Env: autoescape=True, limited caching, basic globals, no auto-reload\n        - relaxed: Default Jinja2 behavior (use with trusted templates only)\n            All default Jinja2 features enabled\n            Env: autoescape=False, full caching, all globals, auto-reload allowed\n\n    Args:\n        security_level: Level of security restrictions (\"strict\", \"standard\", \"relaxed\")\n    \"\"\"\n\n    def __init__(self, security_level: Jinja2SecurityLevel = \"standard\"):\n        # Store security level for error messages\n        self.security_level = security_level\n\n        if security_level == \"strict\":\n            # Most restrictive settings\n            self.env = SandboxedEnvironment(\n                undefined=jinja2.StrictUndefined,\n                trim_blocks=True,\n                lstrip_blocks=True,\n                autoescape=True,  # Force autoescaping\n                cache_size=0,  # Disable caching\n                auto_reload=False,  # Disable auto reload\n            )\n            # Remove all globals\n            self.env.globals.clear()\n\n            # Minimal set of features\n            safe_filters = {\"lower\", \"upper\", \"title\"}\n            safe_tests = {\"defined\", \"undefined\", \"none\"}\n\n        elif security_level == \"standard\":\n            # Balanced settings\n            self.env = SandboxedEnvironment(\n                undefined=jinja2.StrictUndefined,\n                trim_blocks=True,\n                lstrip_blocks=True,\n                autoescape=False,\n                cache_size=100,  # Limited cache\n                auto_reload=False,  # Still no auto reload\n            )\n            # Allow some safe globals\n            self.env.globals.update(\n                {\n                    \"range\": range,  # Useful for iterations\n                    \"dict\": dict,  # Basic dict operations\n                    \"len\": len,  # Length calculations\n                }\n            )\n\n            # Balanced set of features\n            safe_filters = {\n                \"lower\",\n                \"upper\",\n                \"title\",\n                \"capitalize\",\n                \"trim\",\n                \"strip\",\n                \"replace\",\n                \"safe\",\n                \"int\",\n                \"float\",\n                \"join\",\n                \"split\",\n                \"length\",\n            }\n            safe_tests = {\"defined\", \"undefined\", \"none\", \"number\", \"string\", \"sequence\"}\n\n        elif security_level == \"relaxed\":\n            self.env = Environment(\n                undefined=jinja2.StrictUndefined,\n                trim_blocks=True,\n                lstrip_blocks=True,\n                autoescape=False,  # Default Jinja2 behavior\n                cache_size=400,  # Default cache size\n                auto_reload=True,  # Allow auto reload\n            )\n            # Keep all default globals and features\n            return\n        else:\n            raise ValueError(f\"Invalid security level: {security_level}\")\n\n        # Apply security settings for strict and standard modes\n        self._apply_security_settings(safe_filters, safe_tests)\n\n    def _apply_security_settings(self, safe_filters: Set[str], safe_tests: Set[str]) -&gt; None:\n        \"\"\"Apply security settings by removing unsafe filters and tests.\"\"\"\n        # Remove unsafe filters\n        unsafe_filters = set(self.env.filters.keys()) - safe_filters\n        for unsafe in unsafe_filters:\n            self.env.filters.pop(unsafe, None)\n\n        # Remove unsafe tests\n        unsafe_tests = set(self.env.tests.keys()) - safe_tests\n        for unsafe in unsafe_tests:\n            self.env.tests.pop(unsafe, None)\n\n    def populate(self, template_str: str, user_provided_variables: Dict[str, Any]) -&gt; str:\n        \"\"\"Populate the template with given user_provided_variables.\"\"\"\n        try:\n            template = self.env.from_string(template_str)\n            populated = template.render(**user_provided_variables)\n            # Ensure we return a string for mypy\n            return str(populated)\n        except jinja2.TemplateSyntaxError as e:\n            raise ValueError(\n                f\"Invalid template syntax at line {e.lineno}: {str(e)}\\n\" f\"Security level: {self.security_level}\"\n            ) from e\n        except jinja2.UndefinedError as e:\n            raise ValueError(\n                f\"Undefined variable in template: {str(e)}\\n\" \"Make sure all required variables are provided\"\n            ) from e\n        except Exception as e:\n            raise ValueError(f\"Error populating template: {str(e)}\") from e\n\n    def get_variable_names(self, template_str: str) -&gt; Set[str]:\n        \"\"\"Extract variable names from template.\"\"\"\n        try:\n            ast = self.env.parse(template_str)\n            variables = meta.find_undeclared_variables(ast)\n            # Ensure we return a set of strings for mypy\n            return {str(var) for var in variables}\n        except jinja2.TemplateSyntaxError as e:\n            raise ValueError(f\"Invalid template syntax: {str(e)}\") from e\n</code></pre>"},{"location":"reference/populators/#prompt_templates.populators.Jinja2TemplatePopulator.populate","title":"populate","text":"<pre><code>populate(template_str, user_provided_variables)\n</code></pre> <p>Populate the template with given user_provided_variables.</p> Source code in <code>prompt_templates/populators.py</code> <pre><code>def populate(self, template_str: str, user_provided_variables: Dict[str, Any]) -&gt; str:\n    \"\"\"Populate the template with given user_provided_variables.\"\"\"\n    try:\n        template = self.env.from_string(template_str)\n        populated = template.render(**user_provided_variables)\n        # Ensure we return a string for mypy\n        return str(populated)\n    except jinja2.TemplateSyntaxError as e:\n        raise ValueError(\n            f\"Invalid template syntax at line {e.lineno}: {str(e)}\\n\" f\"Security level: {self.security_level}\"\n        ) from e\n    except jinja2.UndefinedError as e:\n        raise ValueError(\n            f\"Undefined variable in template: {str(e)}\\n\" \"Make sure all required variables are provided\"\n        ) from e\n    except Exception as e:\n        raise ValueError(f\"Error populating template: {str(e)}\") from e\n</code></pre>"},{"location":"reference/populators/#prompt_templates.populators.Jinja2TemplatePopulator.get_variable_names","title":"get_variable_names","text":"<pre><code>get_variable_names(template_str)\n</code></pre> <p>Extract variable names from template.</p> Source code in <code>prompt_templates/populators.py</code> <pre><code>def get_variable_names(self, template_str: str) -&gt; Set[str]:\n    \"\"\"Extract variable names from template.\"\"\"\n    try:\n        ast = self.env.parse(template_str)\n        variables = meta.find_undeclared_variables(ast)\n        # Ensure we return a set of strings for mypy\n        return {str(var) for var in variables}\n    except jinja2.TemplateSyntaxError as e:\n        raise ValueError(f\"Invalid template syntax: {str(e)}\") from e\n</code></pre>"},{"location":"reference/prompt_templates/","title":"Prompt templates","text":"<p>This section documents the prompt template classes.</p>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates","title":"prompt_templates.prompt_templates","text":""},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.BasePromptTemplate","title":"BasePromptTemplate","text":"<p>             Bases: <code>ABC</code></p> <p>An abstract base class for prompt templates.</p> <p>This class defines the common interface and shared functionality for all prompt templates. Users should not instantiate this class directly, but instead use TextPromptTemplate or ChatPromptTemplate, which are subclasses of BasePromptTemplate.</p> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>class BasePromptTemplate(ABC):\n    \"\"\"An abstract base class for prompt templates.\n\n    This class defines the common interface and shared functionality for all prompt templates.\n    Users should not instantiate this class directly, but instead use TextPromptTemplate\n    or ChatPromptTemplate, which are subclasses of BasePromptTemplate.\n    \"\"\"\n\n    def __init__(\n        self,\n        template: Union[str, List[Dict[str, Any]]],\n        template_variables: Optional[List[str]] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        client_parameters: Optional[Dict[str, Any]] = None,\n        custom_data: Optional[Dict[str, Any]] = None,\n        populator: PopulatorType = \"jinja2\",\n        jinja2_security_level: Jinja2SecurityLevel = \"standard\",\n    ) -&gt; None:\n        \"\"\"Initialize a prompt template.\n\n        Args:\n            template: The template string or list of message dictionaries.\n            template_variables: List of variables used in the template.\n            metadata: Dictionary of metadata about the template.\n            client_parameters: Dictionary of parameters for the inference client (e.g., temperature, model).\n            custom_data: Dictionary of custom data which does not fit into the other categories.\n            populator: The populator to use. Choose from Literal[\"jinja2\", \"double_brace_regex\", \"single_brace_regex\"]. Defaults to \"jinja2\".\n            jinja2_security_level: Security level for Jinja2 populator. Choose from Literal[\"strict\", \"standard\", \"relaxed\"]. Defaults to \"standard\".\n        \"\"\"\n        # Type validation\n        if template_variables is not None and not isinstance(template_variables, list):\n            raise TypeError(f\"template_variables must be a list, got {type(template_variables).__name__}\")\n        if metadata is not None and not isinstance(metadata, dict):\n            raise TypeError(f\"metadata must be a dict, got {type(metadata).__name__}\")\n        if client_parameters is not None and not isinstance(client_parameters, dict):\n            raise TypeError(f\"client_parameters must be a dict, got {type(client_parameters).__name__}\")\n        if custom_data is not None and not isinstance(custom_data, dict):\n            raise TypeError(f\"custom_data must be a dict, got {type(custom_data).__name__}\")\n\n        # Initialize attributes\n        self.template = template\n        self.template_variables = template_variables or []\n        self.metadata = metadata or {}\n        self.client_parameters = client_parameters or {}\n        self.custom_data = custom_data or {}\n        self.populator = populator\n        self.jinja2_security_level = jinja2_security_level\n\n        # Validate template format\n        self._validate_template_format(self.template)\n\n        # Create populator instance\n        self._create_populator_instance(self.populator, self.jinja2_security_level)\n\n        # Validate that variables provided in template and template_variables are equal\n        if self.template_variables:\n            self._validate_template_variables_equality()\n\n    @abstractmethod\n    def populate(self, **user_provided_variables: Any) -&gt; str | List[Dict[str, Any]]:\n        \"\"\"Abstract method to populate the prompt template with user-provided variables.\n\n        Args:\n            **user_provided_variables: The values to fill placeholders in the template.\n\n        Returns:\n            str | List[Dict[str, Any]]: The populated prompt content.\n        \"\"\"\n        pass\n\n    @classmethod\n    def load_from_local(\n        cls,\n        path: Union[str, Path],\n        populator: PopulatorType = \"jinja2\",\n        jinja2_security_level: Literal[\"strict\", \"standard\", \"relaxed\"] = \"standard\",\n        yaml_library: str = \"ruamel\",\n    ) -&gt; Union[\"TextPromptTemplate\", \"ChatPromptTemplate\"]:\n        \"\"\"Load a prompt template from a local YAML file.\n\n        Args:\n            path (Union[str, Path]): Path to the YAML file containing the prompt template\n            populator ([PopulatorType]): The populator type to use among Literal[\"double_brace_regex\", \"single_brace_regex\", \"jinja2\"]. Defaults to \"jinja2\".\n            jinja2_security_level (Literal[\"strict\", \"standard\", \"relaxed\"], optional): The security level for the Jinja2 populator. Defaults to \"standard\".\n            yaml_library (str, optional): The YAML library to use (\"ruamel\" or \"pyyaml\"). Defaults to \"ruamel\".\n\n        Returns:\n            Union[TextPromptTemplate, ChatPromptTemplate]: The loaded template instance\n\n        Raises:\n            FileNotFoundError: If the file doesn't exist\n            ValueError: If the file is not a .yaml/.yml file\n            ValueError: If the YAML structure is invalid\n            ValueError: If attempting to load a text template with ChatPromptTemplate or vice versa\n\n        Examples:\n            Download a text prompt template:\n            &gt;&gt;&gt; from prompt_templates import TextPromptTemplate\n            &gt;&gt;&gt; prompt_template = TextPromptTemplate.load_from_local(\"./tests/test_data/example_prompts/translate.yaml\")\n            &gt;&gt;&gt; print(prompt_template)\n            TextPromptTemplate(template='Translate the following text to {{language}}:\\\\n{{..., template_variables=['language', 'text'], metadata={'name': 'Simple Translator', 'description': 'A si..., client_parameters={}, custom_data={}, populator='jinja2', jinja2_security_level='standard')\n            &gt;&gt;&gt; prompt_template.template\n            'Translate the following text to {{language}}:\\\\n{{text}}'\n            &gt;&gt;&gt; prompt_template.template_variables\n            ['language', 'text']\n            &gt;&gt;&gt; prompt_template.metadata['name']\n            'Simple Translator'\n\n            Download a chat prompt template:\n            &gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n            &gt;&gt;&gt; prompt_template = ChatPromptTemplate.load_from_local(\"./tests/test_data/example_prompts/code_teacher.yaml\")\n            &gt;&gt;&gt; print(prompt_template)\n            ChatPromptTemplate(template=[{'role': 'system', 'content': 'You are a coding a...', template_variables=['concept', 'programming_language'], metadata={'name': 'Code Teacher', 'description': 'A simple ...', client_parameters={}, custom_data={}, populator='jinja2', jinja2_security_level='standard')\n            &gt;&gt;&gt; prompt_template.template\n            [{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what {{concept}} is in {{programming_language}}.'}]\n            &gt;&gt;&gt; prompt_template.template_variables\n            ['concept', 'programming_language']\n            &gt;&gt;&gt; prompt_template.metadata['version']\n            '0.0.1'\n\n        \"\"\"\n        path = Path(path)\n        if not path.exists():\n            raise FileNotFoundError(f\"Template file not found: {path}\")\n        if path.suffix not in VALID_PROMPT_EXTENSIONS:\n            raise ValueError(f\"Template file must be a .yaml or .yml file, got: {path}\")\n\n        yaml = create_yaml_handler(yaml_library)\n        try:\n            with open(path, \"r\") as file:\n                if yaml_library == \"ruamel\":\n                    prompt_file_dic = yaml.load(file)\n                else:\n                    prompt_file_dic = yaml.safe_load(file)\n        except Exception as e:\n            raise ValueError(\n                f\"Failed to parse '{path}' as a valid YAML file. \"\n                f\"Please ensure the file is properly formatted.\\n\"\n                f\"Error details: {str(e)}\"\n            ) from e\n\n        cls._validate_template_type(prompt_file_dic, str(path))\n\n        return cls._load_template_from_dict(\n            prompt_file_dic, populator=populator, jinja2_security_level=jinja2_security_level\n        )\n\n    @classmethod\n    def load_from_hub(\n        cls,\n        repo_id: str,\n        filename: str,\n        repo_type: str = \"dataset\",\n        revision: Optional[str] = None,\n        populator: PopulatorType = \"jinja2\",\n        jinja2_security_level: Literal[\"strict\", \"standard\", \"relaxed\"] = \"standard\",\n        yaml_library: str = \"ruamel\",\n    ) -&gt; Union[\"TextPromptTemplate\", \"ChatPromptTemplate\"]:\n        \"\"\"Load a prompt template from the Hugging Face Hub.\n\n        Downloads and loads a prompt template from a repository on the Hugging Face Hub.\n        The template file should be a YAML file following the standardized format.\n\n        Args:\n            repo_id (str): The repository ID on Hugging Face Hub (e.g., 'username/repo')\n            filename (str): Name of the YAML file containing the template\n            repo_type (str, optional): Type of repository. Must be one of\n                ['dataset', 'model', 'space']. Defaults to \"dataset\"\n            revision (Optional[str], optional): Git revision to download from.\n                Can be a branch name, tag, or commit hash. Defaults to None\n            populator ([PopulatorType]): The populator type to use among Literal[\"double_brace_regex\", \"single_brace_regex\", \"jinja2\"]. Defaults to \"jinja2\".\n            jinja2_security_level (Literal[\"strict\", \"standard\", \"relaxed\"], optional): The security level for the Jinja2 populator. Defaults to \"standard\".\n            yaml_library (str, optional): The YAML library to use (\"ruamel\" or \"pyyaml\"). Defaults to \"ruamel\".\n\n\n        Returns:\n            Union[TextPromptTemplate, ChatPromptTemplate]: The loaded template instance\n\n        Raises:\n            ValueError: If repo_id format is invalid\n            ValueError: If repo_type is invalid\n            FileNotFoundError: If file cannot be downloaded from Hub\n            ValueError: If the YAML structure is invalid\n            ValueError: If attempting to load a text template with ChatPromptTemplate or vice versa\n\n        Examples:\n            Download a text prompt template:\n            &gt;&gt;&gt; from prompt_templates import TextPromptTemplate\n            &gt;&gt;&gt; prompt_template = TextPromptTemplate.load_from_hub(\n            ...     repo_id=\"MoritzLaurer/example_prompts\",\n            ...     filename=\"translate.yaml\"\n            ... )\n            &gt;&gt;&gt; print(prompt_template)\n            TextPromptTemplate(template='Translate the following text to {{language}}:\\\\n{{...', template_variables=['language', 'text'], metadata={'name': 'Simple Translator', 'description': 'A si...', client_parameters={}, custom_data={}, populator='jinja2', jinja2_security_level='standard')\n            &gt;&gt;&gt; prompt_template.template\n            'Translate the following text to {{language}}:\\\\n{{text}}'\n            &gt;&gt;&gt; prompt_template.template_variables\n            ['language', 'text']\n            &gt;&gt;&gt; prompt_template.metadata['name']\n            'Simple Translator'\n\n            Download a chat prompt template:\n            &gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n            &gt;&gt;&gt; prompt_template = ChatPromptTemplate.load_from_hub(\n            ...     repo_id=\"MoritzLaurer/example_prompts\",\n            ...     filename=\"code_teacher.yaml\"\n            ... )\n            &gt;&gt;&gt; print(prompt_template)\n            ChatPromptTemplate(template=[{'role': 'system', 'content': 'You are a coding a...', template_variables=['concept', 'programming_language'], metadata={'name': 'Code Teacher', 'description': 'A simple ...', client_parameters={}, custom_data={}, populator='jinja2', jinja2_security_level='standard')\n            &gt;&gt;&gt; prompt_template.template\n            [{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what {{concept}} is in {{programming_language}}.'}]\n            &gt;&gt;&gt; prompt_template.template_variables\n            ['concept', 'programming_language']\n            &gt;&gt;&gt; prompt_template.metadata['version']\n            '0.0.1'\n        \"\"\"\n        # Validate Hub parameters\n        try:\n            validate_repo_id(repo_id)\n        except ValueError as e:\n            raise ValueError(f\"Invalid repo_id format: {str(e)}\") from e\n\n        if repo_type not in [\"dataset\", \"model\", \"space\"]:\n            raise ValueError(f\"repo_type must be one of ['dataset', 'model', 'space'], got {repo_type}\")\n\n        # Ensure .yaml extension\n        if not filename.endswith(VALID_PROMPT_EXTENSIONS):\n            filename += \".yaml\"\n\n        try:\n            file_path = hf_hub_download(repo_id=repo_id, filename=filename, repo_type=repo_type, revision=revision)\n        except Exception as e:\n            raise FileNotFoundError(f\"Failed to download template from Hub: {str(e)}\") from e\n\n        yaml = create_yaml_handler(yaml_library)\n        try:\n            with open(file_path, \"r\") as file:\n                if yaml_library == \"ruamel\":\n                    prompt_file_dic = yaml.load(file)\n                else:\n                    prompt_file_dic = yaml.safe_load(file)\n        except Exception as e:\n            raise ValueError(\n                f\"Failed to parse '{filename}' as a valid YAML file. \"\n                f\"Please ensure the file is properly formatted.\\n\"\n                f\"Error details: {str(e)}\"\n            ) from e\n\n        file_info = f\"'{filename}' from '{repo_id}'\"\n        cls._validate_template_type(prompt_file_dic, file_info)\n\n        return cls._load_template_from_dict(\n            prompt_file_dic, populator=populator, jinja2_security_level=jinja2_security_level\n        )\n\n    @staticmethod\n    def _load_template_from_dict(\n        prompt_file_dic: Dict[str, Any],\n        populator: PopulatorType = \"jinja2\",\n        jinja2_security_level: Literal[\"strict\", \"standard\", \"relaxed\"] = \"standard\",\n    ) -&gt; Union[\"TextPromptTemplate\", \"ChatPromptTemplate\"]:\n        \"\"\"Internal method to load a template from parsed YAML data.\n\n        Args:\n            prompt_file_dic: Dictionary containing parsed YAML data\n            populator: Optional template populator type\n            jinja2_security_level: Security level for Jinja2 populator\n\n        Returns:\n            Union[TextPromptTemplate, ChatPromptTemplate]: Loaded template instance\n\n        Raises:\n            ValueError: If YAML structure is invalid\n        \"\"\"\n        # Validate YAML structure\n        if \"prompt\" not in prompt_file_dic:\n            raise ValueError(\n                f\"Invalid YAML structure: The top-level keys are {list(prompt_file_dic.keys())}. \"\n                \"The YAML file must contain the key 'prompt' as the top-level key.\"\n            )\n\n        prompt_data = prompt_file_dic[\"prompt\"]\n\n        # Check for standard \"template\" key\n        if \"template\" not in prompt_data:\n            if \"messages\" in prompt_data:\n                template = prompt_data[\"messages\"]\n                del prompt_data[\"messages\"]\n                logger.info(\n                    \"The YAML file uses the 'messages' key for the chat prompt template following the LangChain format. \"\n                    \"The 'messages' key is renamed to 'template' for simplicity and consistency in this library.\"\n                )\n            else:\n                raise ValueError(\n                    f\"Invalid YAML structure under 'prompt' key: {list(prompt_data.keys())}. \"\n                    \"The YAML file must contain a 'template' key under 'prompt'. \"\n                    \"Please refer to the documentation for a compatible YAML example.\"\n                )\n        else:\n            template = prompt_data[\"template\"]\n\n        # Extract fields\n        template_variables = prompt_data.get(\"template_variables\")\n        metadata = prompt_data.get(\"metadata\")\n        client_parameters = prompt_data.get(\"client_parameters\")\n        custom_data = {\n            k: v\n            for k, v in prompt_data.items()\n            if k not in [\"template\", \"template_variables\", \"metadata\", \"client_parameters\", \"custom_data\"]\n        }\n        custom_data = {**prompt_data.get(\"custom_data\", {}), **custom_data}\n\n        # Determine template type and create appropriate instance\n        if isinstance(template, list) and any(isinstance(item, dict) for item in template):\n            return ChatPromptTemplate(\n                template=template,\n                template_variables=template_variables,\n                metadata=metadata,\n                client_parameters=client_parameters,\n                custom_data=custom_data,\n                populator=populator,\n                jinja2_security_level=jinja2_security_level,\n            )\n        elif isinstance(template, str):\n            return TextPromptTemplate(\n                template=template,\n                template_variables=template_variables,\n                metadata=metadata,\n                client_parameters=client_parameters,\n                custom_data=custom_data,\n                populator=populator,\n                jinja2_security_level=jinja2_security_level,\n            )\n        else:\n            raise ValueError(\n                f\"Invalid template type: {type(template)}. \"\n                \"Template must be either a string for text prompts or a list of dictionaries for chat prompts.\"\n            )\n\n    @classmethod\n    def _validate_template_type(cls, prompt_file_dic: Dict[str, Any], file_info: str) -&gt; None:\n        \"\"\"Validate that the template type matches the class it was called from.\n\n        Args:\n            prompt_file_dic: Dictionary containing parsed YAML data\n            file_info: String describing the file location (e.g., file path or Hub location)\n                for error messages\n\n        Raises:\n            ValueError: If template structure is invalid or type doesn't match the class\n        \"\"\"\n        if not isinstance(prompt_file_dic, dict) or \"prompt\" not in prompt_file_dic:\n            raise ValueError(f\"File '{file_info}' must contain a top-level 'prompt' key\")\n\n        template = prompt_file_dic[\"prompt\"].get(\"template\")\n        if template is None:\n            raise ValueError(f\"Template is missing in file '{file_info}'\")\n\n        is_chat_template = isinstance(template, list) and any(isinstance(item, dict) for item in template)\n        is_text_template = isinstance(template, str)\n\n        if cls.__name__ == \"ChatPromptTemplate\" and not is_chat_template:\n            raise ValueError(\n                f\"Cannot load a text template using ChatPromptTemplate. The template in '{file_info}' \"\n                \"appears to be a text template. Use TextPromptTemplate.load_from_local() or .load_from_hub() instead.\"\n            )\n        elif cls.__name__ == \"TextPromptTemplate\" and not is_text_template:\n            raise ValueError(\n                f\"Cannot load a chat template using TextPromptTemplate. The template in {file_info} \"\n                \"appears to be a chat template. Use ChatPromptTemplate.load_from_local() or .load_from_hub() instead.\"\n            )\n\n    def save_to_local(\n        self,\n        path: Union[str, Path],\n        format: Optional[Literal[\"yaml\", \"json\"]] = None,\n        yaml_library: str = \"ruamel\",\n        prettify_template: bool = True,\n    ) -&gt; None:\n        \"\"\"Save the prompt template as a local YAML or JSON file.\n\n        Args:\n            path: Path where to save the file. Can be string or Path object\n            format: Output format (\"yaml\" or \"json\"). If None, inferred from filename\n            yaml_library: YAML library to use (\"ruamel\" or \"pyyaml\"). Defaults to \"ruamel\" for better formatting and format preservation.\n            prettify_template: If true format the template content with literal block scalars, i.e. \"|-\" in yaml.\n                This makes the string behave like a Python '''...''' block to make strings easier to read and edit.\n                Defaults to True\n\n        Examples:\n            &gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n            &gt;&gt;&gt; messages_template = [\n            ...     {\"role\": \"system\", \"content\": \"You are a coding assistant who explains concepts clearly and provides short examples.\"},\n            ...     {\"role\": \"user\", \"content\": \"Explain what {{concept}} is in {{programming_language}}.\"}\n            ... ]\n            &gt;&gt;&gt; template_variables = [\"concept\", \"programming_language\"]\n            &gt;&gt;&gt; metadata = {\n            ...     \"name\": \"Code Teacher\",\n            ...     \"description\": \"A simple chat prompt for explaining programming concepts with examples\",\n            ...     \"tags\": [\"programming\", \"education\"],\n            ...     \"version\": \"0.0.1\",\n            ...     \"author\": \"My Awesome Company\"\n            ... }\n            &gt;&gt;&gt; prompt_template = ChatPromptTemplate(\n            ...     template=messages_template,\n            ...     template_variables=template_variables,\n            ...     metadata=metadata,\n            ... )\n            &gt;&gt;&gt; prompt_template.save_to_local(\"./tests/test_data/example_prompts/code_teacher_test.yaml\")  # doctest: +SKIP\n        \"\"\"\n\n        path = Path(path)\n        # Handle format inference and validation\n        file_extension = path.suffix.lstrip(\".\")\n        if format is None:\n            # Infer format from extension\n            if file_extension in [\"yaml\", \"yml\"]:\n                format = \"yaml\"\n            elif file_extension == \"json\":\n                format = \"json\"\n            else:\n                raise ValueError(f\"Cannot infer format from file extension: {path.suffix}\")\n        else:\n            # Validate explicitly provided format matches file extension\n            if format not in [\"yaml\", \"yml\", \"json\"]:\n                raise ValueError(f\"Unsupported format: {format}\")\n            if format in [\"yaml\", \"yml\"] and file_extension in [\"yaml\", \"yml\"]:\n                # Both are YAML variants, so they match\n                pass\n            elif format != file_extension:\n                raise ValueError(f\"Provided format '{format}' does not match file extension '{path.suffix}'\")\n\n        data = {\n            \"prompt\": {\n                \"template\": self.template,\n                \"template_variables\": self.template_variables,\n                \"metadata\": self.metadata,\n                \"client_parameters\": self.client_parameters,\n                \"custom_data\": self.custom_data,\n            }\n        }\n\n        if prettify_template:\n            data[\"prompt\"][\"template\"] = format_template_content(data[\"prompt\"][\"template\"])\n\n        with open(path, \"w\", encoding=\"utf-8\") as f:\n            if format == \"json\":\n                json.dump(data, f, indent=2, ensure_ascii=False)\n            else:  # yaml\n                yaml_handler = create_yaml_handler(yaml_library)\n                if yaml_library == \"ruamel\":\n                    yaml_handler.dump(data, f)\n                elif yaml_library == \"pyyaml\":\n                    yaml_handler.dump(data, f, sort_keys=False, allow_unicode=True)\n                else:\n                    raise ValueError(\n                        f\"Unknown yaml library: {yaml_library}. Valid options are: 'ruamel' (default) or 'pyyaml'.\"\n                    )\n\n    def save_to_hub(\n        self,\n        repo_id: str,\n        filename: str,\n        repo_type: str = \"dataset\",\n        format: Optional[Literal[\"yaml\", \"json\"]] = None,\n        yaml_library: str = \"ruamel\",\n        prettify_template: bool = True,\n        token: Optional[str] = None,\n        create_repo: bool = False,\n        private: bool = False,\n        resource_group_id: Optional[str] = None,\n        revision: Optional[str] = None,\n        create_pr: bool = False,\n        commit_message: Optional[str] = None,\n        commit_description: Optional[str] = None,\n        parent_commit: Optional[str] = None,\n    ) -&gt; CommitInfo:\n        \"\"\"Save the prompt template to the Hugging Face Hub as a YAML or JSON file.\n\n        Args:\n            repo_id: The repository ID on the Hugging Face Hub (e.g., \"username/repo-name\")\n            filename: Name of the file to save (e.g., \"prompt.yaml\" or \"prompt.json\")\n            repo_type: Type of repository (\"dataset\", \"model\", or \"space\"). Defaults to \"dataset\"\n            token: Hugging Face API token. If None, will use token from environment\n            commit_message: Custom commit message. If None, uses default message\n            create_repo: Whether to create the repository if it doesn't exist. Defaults to False\n            format: Output format (\"yaml\" or \"json\"). If None, inferred from filename extension\n            yaml_library: YAML library to use (\"ruamel\" or \"pyyaml\"). Defaults to \"ruamel\" for better formatting and format preservation.\n            prettify_template: If true format the template content with literal block scalars, i.e. \"|-\" in yaml.\n                This makes the string behave like a Python '''...''' block to make strings easier to read and edit.\n                Defaults to True\n            private: Whether to create a private repository. Defaults to False\n            resource_group_id: Optional resource group ID to associate with the repository\n            revision: Optional branch/revision to push to. Defaults to main branch\n            create_pr: Whether to create a Pull Request instead of pushing directly. Defaults to False\n            commit_description: Optional commit description\n            parent_commit: Optional parent commit to create PR from\n\n        Returns:\n            CommitInfo: Information about the commit/PR\n\n        Examples:\n            &gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n            &gt;&gt;&gt; messages_template = [\n            ...     {\"role\": \"system\", \"content\": \"You are a coding assistant who explains concepts clearly and provides short examples.\"},\n            ...     {\"role\": \"user\", \"content\": \"Explain what {{concept}} is in {{programming_language}}.\"}\n            ... ]\n            &gt;&gt;&gt; template_variables = [\"concept\", \"programming_language\"]\n            &gt;&gt;&gt; metadata = {\n            ...     \"name\": \"Code Teacher\",\n            ...     \"description\": \"A simple chat prompt for explaining programming concepts with examples\",\n            ...     \"tags\": [\"programming\", \"education\"],\n            ...     \"version\": \"0.0.1\",\n            ...     \"author\": \"My Awesome Company\"\n            ... }\n            &gt;&gt;&gt; prompt_template = ChatPromptTemplate(\n            ...     template=messages_template,\n            ...     template_variables=template_variables,\n            ...     metadata=metadata,\n            ... )\n            &gt;&gt;&gt; prompt_template.save_to_hub(  # doctest: +SKIP\n            ...     repo_id=\"MoritzLaurer/example_prompts_test\",\n            ...     filename=\"code_teacher_test.yaml\",\n            ...     #create_repo=True,  # if the repo does not exist, create it\n            ...     #private=True,  # if you want to create a private repo\n            ...     #token=\"hf_...\"\n            ... )\n        \"\"\"\n\n        # Handle format inference and validation\n        if format is None:\n            # Infer format from extension\n            extension = Path(filename).suffix.lstrip(\".\")\n            if extension in [\"yaml\", \"yml\"]:\n                format = \"yaml\"\n            elif extension == \"json\":\n                format = \"json\"\n            else:\n                format = \"yaml\"  # default if no extension\n                filename += \".yaml\"\n        else:\n            # Validate explicitly provided format matches file extension\n            if format not in [\"yaml\", \"yml\", \"json\"]:\n                raise ValueError(f\"Unsupported format: {format}\")\n\n            file_extension = Path(filename).suffix.lstrip(\".\")\n            if format in [\"yaml\", \"yml\"] and file_extension in [\"yaml\", \"yml\"]:\n                # Both are YAML variants, so they match\n                pass\n            elif format != file_extension:\n                raise ValueError(f\"Provided format '{format}' does not match file extension '{filename}'\")\n\n        # Convert template to the specified format\n        data = {\n            \"prompt\": {\n                \"template\": self.template,\n                \"template_variables\": self.template_variables,\n                \"metadata\": self.metadata,\n                \"client_parameters\": self.client_parameters,\n                \"custom_data\": self.custom_data,\n            }\n        }\n\n        if prettify_template:\n            data[\"prompt\"][\"template\"] = format_template_content(data[\"prompt\"][\"template\"])\n\n        if format == \"json\":\n            content = json.dumps(data, indent=2, ensure_ascii=False)\n            content_bytes = content.encode(\"utf-8\")\n        else:  # yaml\n            yaml_handler = create_yaml_handler(yaml_library)\n            string_stream = io.StringIO()\n            yaml_handler.dump(data, string_stream)\n            content = string_stream.getvalue()\n            content_bytes = content.encode(\"utf-8\")\n\n        # Upload to Hub\n        api = HfApi(token=token)\n\n        # Check if repo exists before attempting to create it to avoid overwriting repo card\n        try:\n            api.repo_info(repo_id=repo_id, repo_type=repo_type)\n            repo_exists = True\n        except RepositoryNotFoundError:\n            repo_exists = False\n\n        if create_repo and repo_exists:\n            logger.info(\n                f\"You specified create_repo={create_repo}, but repository {repo_id} already exists. \"\n                \"Skipping repo creation.\"\n            )\n        elif not create_repo and not repo_exists:\n            raise ValueError(f\"Repository {repo_id} does not exist. Set create_repo=True to create it.\")\n        elif create_repo and not repo_exists:\n            logger.info(f\"Creating/Updating HF Hub repository {repo_id}\")\n            api.create_repo(\n                repo_id=repo_id,\n                repo_type=repo_type,\n                token=token,\n                private=private,\n                # exist_ok=exist_ok,  # not using this arg to avoid inconsistency\n                resource_group_id=resource_group_id,\n            )\n            repocard_text = (\n                \"---\\n\"\n                \"library_name: prompt-templates\\n\"\n                \"tags:\\n\"\n                \"- prompts\\n\"\n                \"- prompt-templates\\n\"\n                \"---\\n\"\n                \"This repository was created with the `prompt-templates` library and contains\\n\"\n                \"prompt templates in the `Files` tab.\\n\"\n                \"For easily reusing these templates, see the [documentation](https://github.com/MoritzLaurer/prompt-templates).\"\n            )\n            card = RepoCard(repocard_text)\n            card.push_to_hub(\n                repo_id=repo_id,\n                repo_type=repo_type,\n                token=token,\n                commit_message=\"Create/Update repo card with prompt-templates library\",\n                create_pr=create_pr,\n                parent_commit=parent_commit,\n            )\n        elif not create_repo and repo_exists:\n            # Update repo metadata to make prompt templates discoverable on the HF Hub\n            logger.info(f\"Updating HF Hub repository {repo_id} with prompt-templates library metadata.\")\n            metadata_update(\n                repo_id=repo_id,\n                metadata={\"library_name\": \"prompt-templates\", \"tags\": [\"prompts\", \"prompt-templates\"]},\n                repo_type=repo_type,\n                overwrite=False,\n                token=token,\n                commit_message=commit_message or \"Update repo metadata with prompt-templates library\",\n                commit_description=commit_description,\n                revision=revision,\n                create_pr=create_pr,\n                parent_commit=parent_commit,\n            )\n\n        # Upload file\n        logger.info(f\"Uploading prompt template {filename} to HF Hub repository {repo_id}\")\n        return api.upload_file(\n            path_or_fileobj=io.BytesIO(content_bytes),\n            path_in_repo=filename,\n            repo_id=repo_id,\n            repo_type=repo_type,\n            token=token,\n            commit_message=commit_message or f\"Upload prompt template {filename}\",\n            commit_description=commit_description,\n            revision=revision,\n            create_pr=create_pr,\n            parent_commit=parent_commit,\n        )\n\n    def display(self, format: Literal[\"json\", \"yaml\"] = \"json\") -&gt; None:\n        \"\"\"Display the prompt configuration in the specified format.\n\n        Examples:\n            &gt;&gt;&gt; from prompt_templates import TextPromptTemplate\n            &gt;&gt;&gt; prompt_template = TextPromptTemplate.load_from_hub(\n            ...     repo_id=\"MoritzLaurer/example_prompts\",\n            ...     filename=\"translate.yaml\"\n            ... )\n            &gt;&gt;&gt; prompt_template.display(format=\"yaml\")  # doctest: +NORMALIZE_WHITESPACE\n            template: |-\n              Translate the following text to {{language}}:\n              {{text}}\n            template_variables:\n            - language\n            - text\n            metadata:\n              name: \"Simple Translator\"\n              description: \"A simple translation prompt for illustrating the standard prompt YAML\n                format\"\n              tags:\n              - translation\n              - multilinguality\n              version: \"0.0.1\"\n              author: \"Guy van Babel\"\n            client_parameters: {}\n            custom_data: {}\n        \"\"\"\n        # Create a clean dict with only the relevant attributes\n        display_dict = {\n            \"template\": self.template,\n            \"template_variables\": self.template_variables,\n            \"metadata\": self.metadata,\n            \"client_parameters\": self.client_parameters,\n            \"custom_data\": self.custom_data,\n        }\n\n        if format == \"json\":\n            print(json.dumps(display_dict, indent=2), end=\"\")\n        elif format == \"yaml\":\n            # Create a new YAML instance without version/tags output\n            yaml_handler = YAML()\n            yaml_handler.explicit_start = False\n            yaml_handler.explicit_end = False\n            yaml_handler.version = None\n\n            # Dump to string first to avoid stdout formatting issues\n            output = io.StringIO()\n            yaml_handler.dump(display_dict, output)\n            print(output.getvalue(), end=\"\")\n\n    def __getitem__(self, key: str) -&gt; Any:\n        return self.__dict__[key]\n\n    def __repr__(self) -&gt; str:\n        # Filter out private attributes (those starting with _)\n        public_attrs = {k: v for k, v in self.__dict__.items() if not k.startswith(\"_\")}\n\n        attributes = \", \".join(\n            f\"{key}={repr(value)[:50]}...'\" if len(repr(value)) &gt; 50 else f\"{key}={repr(value)}\"\n            for key, value in public_attrs.items()\n        )\n        return f\"{self.__class__.__name__}({attributes})\"\n\n    def _populate_placeholders(self, template_part: Any, user_provided_variables: Dict[str, Any]) -&gt; Any:\n        \"\"\"Recursively fill placeholders in strings or nested structures like dicts or lists.\"\"\"\n        if isinstance(template_part, str):\n            # fill placeholders in strings\n            return self._populator_instance.populate(template_part, user_provided_variables)\n        elif isinstance(template_part, dict):\n            # Recursively handle dictionaries\n            return {\n                key: self._populate_placeholders(value, user_provided_variables)\n                for key, value in template_part.items()\n            }\n\n        elif isinstance(template_part, list):\n            # Recursively handle lists\n            return [self._populate_placeholders(item, user_provided_variables) for item in template_part]\n\n        return template_part  # For non-string, non-dict, non-list types, return as is\n\n    def _validate_user_provided_variables(self, user_provided_variables: Dict[str, Any]) -&gt; None:\n        \"\"\"Validate that all required variables are provided by the user.\n\n        Args:\n            user_provided_variables: Variables provided by user to populate template\n\n        Raises:\n            ValueError: If validation fails\n        \"\"\"\n        # We know that template variables and template_variables are equal based on _validate_template_variables_equality, so we can validate against either\n        required_variables = (\n            set(self.template_variables) if self.template_variables else self._get_variables_in_template()\n        )\n        provided_variables = set(user_provided_variables.keys())\n\n        # Check for missing and unexpected variables\n        missing_vars = required_variables - provided_variables\n        unexpected_vars = provided_variables - required_variables\n\n        if missing_vars or unexpected_vars:\n            error_parts = []\n\n            if missing_vars:\n                error_parts.append(\n                    f\"Missing required variables:\\n\"\n                    f\"  Required: {sorted(missing_vars)}\\n\"\n                    f\"  Provided: {sorted(provided_variables)}\"\n                )\n\n            if unexpected_vars:\n                error_parts.append(\n                    f\"Unexpected variables provided:\\n\"\n                    f\"  Expected required variables: {sorted(required_variables)}\\n\"\n                    f\"  Extra variables: {sorted(unexpected_vars)}\"\n                )\n\n            raise ValueError(\"\\n\".join(error_parts))\n\n    def _validate_template_variables_equality(self) -&gt; None:\n        \"\"\"Validate that the declared template_variables and the actual variables in the template are identical.\"\"\"\n        variables_in_template = self._get_variables_in_template()\n        template_variables = set(self.template_variables or [])\n\n        # Check for mismatches\n        undeclared_template_variables = variables_in_template - template_variables\n        unused_template_variables = template_variables - variables_in_template\n\n        if undeclared_template_variables or unused_template_variables:\n            error_parts = []\n\n            if undeclared_template_variables:\n                error_parts.append(\n                    f\"template contains variables that are not declared in template_variables: {list(undeclared_template_variables)}\"\n                )\n            if unused_template_variables:\n                error_parts.append(\n                    f\"template_variables declares variables that are not used in template: {list(unused_template_variables)}\"\n                )\n\n            template_extract = (\n                str(self.template)[:100] + \"...\" if len(str(self.template)) &gt; 100 else str(self.template)\n            )\n            error_parts.append(f\"Template extract: {template_extract}\")\n\n            raise ValueError(\"\\n\".join(error_parts))\n\n    def _get_variables_in_template(self) -&gt; Set[str]:\n        \"\"\"Get all variables used as placeholders in the template string or messages dictionary.\"\"\"\n        variables_in_template = set()\n        if isinstance(self.template, str):\n            variables_in_template = self._populator_instance.get_variable_names(self.template)\n        elif isinstance(self.template, list) and any(isinstance(item, dict) for item in self.template):\n            for message in self.template:\n                content = message[\"content\"]\n                if isinstance(content, str):\n                    variables_in_template.update(self._populator_instance.get_variable_names(content))\n                elif isinstance(content, list):\n                    # Recursively search for variables in nested content\n                    for item in content:\n                        variables_in_template.update(self._get_variables_in_dict(item))\n        return variables_in_template\n\n    def _get_variables_in_dict(self, d: Dict[str, Any]) -&gt; Set[str]:\n        \"\"\"Recursively extract variables from a dictionary structure.\"\"\"\n        variables = set()\n        for value in d.values():\n            if isinstance(value, str):\n                variables.update(self._populator_instance.get_variable_names(value))\n            elif isinstance(value, dict):\n                variables.update(self._get_variables_in_dict(value))\n            elif isinstance(value, list):\n                for item in value:\n                    if isinstance(item, dict):\n                        variables.update(self._get_variables_in_dict(item))\n        return variables\n\n    def _validate_template_format(self, template: Union[str, List[Dict[str, Any]]]) -&gt; None:\n        \"\"\"Validate the format of the template at initialization.\"\"\"\n        if isinstance(template, list):\n            if not all(isinstance(msg, dict) for msg in template):\n                raise ValueError(\"All messages in template must be dictionaries\")\n\n            required_keys = {\"role\", \"content\"}\n            for msg in template:\n                missing_keys = required_keys - set(msg.keys())\n                if missing_keys:\n                    raise ValueError(\n                        f\"Each message must have a 'role' and a 'content' key. Missing keys: {missing_keys}\"\n                    )\n\n                if not isinstance(msg[\"role\"], str):\n                    raise ValueError(\"Message 'role' must be a string\")\n\n                # Allow content to be either a string or a list of content items\n                if not isinstance(msg[\"content\"], (str, list)):\n                    raise ValueError(\"Message 'content' must be either a string or a list\")\n\n                # If content is a list, validate each item\n                # Can be list if passing images to OpenAI API\n                if isinstance(msg[\"content\"], list):\n                    for item in msg[\"content\"]:\n                        if not isinstance(item, dict):\n                            raise ValueError(\"Each content item in a list must be a dictionary\")\n\n                if msg[\"role\"] not in {\"system\", \"user\", \"assistant\"}:\n                    raise ValueError(f\"Invalid role '{msg['role']}'. Must be one of: system, user, assistant\")\n\n    def _create_populator_instance(self, populator: PopulatorType, jinja2_security_level: Jinja2SecurityLevel) -&gt; None:\n        \"\"\"Create populator instance.\n\n        Args:\n            populator: Explicit populator type. Must be one of ('jinja2', 'double_brace_regex', 'single_brace_regex').\n            jinja2_security_level: Security level for Jinja2 populator\n\n        Raises:\n            ValueError: If an unknown populator type is specified\n        \"\"\"\n        self._populator_instance: TemplatePopulator\n\n        if populator == \"jinja2\":\n            self._populator_instance = Jinja2TemplatePopulator(security_level=jinja2_security_level)\n        elif populator == \"double_brace_regex\":\n            self._populator_instance = DoubleBracePopulator()\n        elif populator == \"single_brace_regex\":\n            self._populator_instance = SingleBracePopulator()\n        else:\n            raise ValueError(\n                f\"Unknown populator type: {populator}. Valid options are: jinja2, double_brace_regex, single_brace_regex\"\n            )\n\n    def __eq__(self, other: Any) -&gt; bool:\n        if not isinstance(other, BasePromptTemplate):\n            return False\n\n        return (\n            self.template == other.template\n            and self.template_variables == other.template_variables\n            and self.metadata == other.metadata\n            and self.client_parameters == other.client_parameters\n            and self.custom_data == other.custom_data\n            and self.populator == other.populator\n        )\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.BasePromptTemplate.__init__","title":"__init__","text":"<pre><code>__init__(template, template_variables=None, metadata=None, client_parameters=None, custom_data=None, populator='jinja2', jinja2_security_level='standard')\n</code></pre> <p>Initialize a prompt template.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>Union[str, List[Dict[str, Any]]]</code> <p>The template string or list of message dictionaries.</p> required <code>template_variables</code> <code>Optional[List[str]]</code> <p>List of variables used in the template.</p> <code>None</code> <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of metadata about the template.</p> <code>None</code> <code>client_parameters</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of parameters for the inference client (e.g., temperature, model).</p> <code>None</code> <code>custom_data</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of custom data which does not fit into the other categories.</p> <code>None</code> <code>populator</code> <code>PopulatorType</code> <p>The populator to use. Choose from Literal[\"jinja2\", \"double_brace_regex\", \"single_brace_regex\"]. Defaults to \"jinja2\".</p> <code>'jinja2'</code> <code>jinja2_security_level</code> <code>Jinja2SecurityLevel</code> <p>Security level for Jinja2 populator. Choose from Literal[\"strict\", \"standard\", \"relaxed\"]. Defaults to \"standard\".</p> <code>'standard'</code> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>def __init__(\n    self,\n    template: Union[str, List[Dict[str, Any]]],\n    template_variables: Optional[List[str]] = None,\n    metadata: Optional[Dict[str, Any]] = None,\n    client_parameters: Optional[Dict[str, Any]] = None,\n    custom_data: Optional[Dict[str, Any]] = None,\n    populator: PopulatorType = \"jinja2\",\n    jinja2_security_level: Jinja2SecurityLevel = \"standard\",\n) -&gt; None:\n    \"\"\"Initialize a prompt template.\n\n    Args:\n        template: The template string or list of message dictionaries.\n        template_variables: List of variables used in the template.\n        metadata: Dictionary of metadata about the template.\n        client_parameters: Dictionary of parameters for the inference client (e.g., temperature, model).\n        custom_data: Dictionary of custom data which does not fit into the other categories.\n        populator: The populator to use. Choose from Literal[\"jinja2\", \"double_brace_regex\", \"single_brace_regex\"]. Defaults to \"jinja2\".\n        jinja2_security_level: Security level for Jinja2 populator. Choose from Literal[\"strict\", \"standard\", \"relaxed\"]. Defaults to \"standard\".\n    \"\"\"\n    # Type validation\n    if template_variables is not None and not isinstance(template_variables, list):\n        raise TypeError(f\"template_variables must be a list, got {type(template_variables).__name__}\")\n    if metadata is not None and not isinstance(metadata, dict):\n        raise TypeError(f\"metadata must be a dict, got {type(metadata).__name__}\")\n    if client_parameters is not None and not isinstance(client_parameters, dict):\n        raise TypeError(f\"client_parameters must be a dict, got {type(client_parameters).__name__}\")\n    if custom_data is not None and not isinstance(custom_data, dict):\n        raise TypeError(f\"custom_data must be a dict, got {type(custom_data).__name__}\")\n\n    # Initialize attributes\n    self.template = template\n    self.template_variables = template_variables or []\n    self.metadata = metadata or {}\n    self.client_parameters = client_parameters or {}\n    self.custom_data = custom_data or {}\n    self.populator = populator\n    self.jinja2_security_level = jinja2_security_level\n\n    # Validate template format\n    self._validate_template_format(self.template)\n\n    # Create populator instance\n    self._create_populator_instance(self.populator, self.jinja2_security_level)\n\n    # Validate that variables provided in template and template_variables are equal\n    if self.template_variables:\n        self._validate_template_variables_equality()\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.BasePromptTemplate.populate","title":"populate  <code>abstractmethod</code>","text":"<pre><code>populate(**user_provided_variables)\n</code></pre> <p>Abstract method to populate the prompt template with user-provided variables.</p> <p>Parameters:</p> Name Type Description Default <code>**user_provided_variables</code> <code>Any</code> <p>The values to fill placeholders in the template.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str | List[Dict[str, Any]]</code> <p>str | List[Dict[str, Any]]: The populated prompt content.</p> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>@abstractmethod\ndef populate(self, **user_provided_variables: Any) -&gt; str | List[Dict[str, Any]]:\n    \"\"\"Abstract method to populate the prompt template with user-provided variables.\n\n    Args:\n        **user_provided_variables: The values to fill placeholders in the template.\n\n    Returns:\n        str | List[Dict[str, Any]]: The populated prompt content.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.BasePromptTemplate.load_from_local","title":"load_from_local  <code>classmethod</code>","text":"<pre><code>load_from_local(path, populator='jinja2', jinja2_security_level='standard', yaml_library='ruamel')\n</code></pre> <p>Load a prompt template from a local YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to the YAML file containing the prompt template</p> required <code>populator</code> <code>[PopulatorType]</code> <p>The populator type to use among Literal[\"double_brace_regex\", \"single_brace_regex\", \"jinja2\"]. Defaults to \"jinja2\".</p> <code>'jinja2'</code> <code>jinja2_security_level</code> <code>Literal['strict', 'standard', 'relaxed']</code> <p>The security level for the Jinja2 populator. Defaults to \"standard\".</p> <code>'standard'</code> <code>yaml_library</code> <code>str</code> <p>The YAML library to use (\"ruamel\" or \"pyyaml\"). Defaults to \"ruamel\".</p> <code>'ruamel'</code> <p>Returns:</p> Type Description <code>Union[TextPromptTemplate, ChatPromptTemplate]</code> <p>Union[TextPromptTemplate, ChatPromptTemplate]: The loaded template instance</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file doesn't exist</p> <code>ValueError</code> <p>If the file is not a .yaml/.yml file</p> <code>ValueError</code> <p>If the YAML structure is invalid</p> <code>ValueError</code> <p>If attempting to load a text template with ChatPromptTemplate or vice versa</p> <p>Examples:</p> <p>Download a text prompt template:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import TextPromptTemplate\n&gt;&gt;&gt; prompt_template = TextPromptTemplate.load_from_local(\"./tests/test_data/example_prompts/translate.yaml\")\n&gt;&gt;&gt; print(prompt_template)\nTextPromptTemplate(template='Translate the following text to {{language}}:\\n{{..., template_variables=['language', 'text'], metadata={'name': 'Simple Translator', 'description': 'A si..., client_parameters={}, custom_data={}, populator='jinja2', jinja2_security_level='standard')\n&gt;&gt;&gt; prompt_template.template\n'Translate the following text to {{language}}:\\n{{text}}'\n&gt;&gt;&gt; prompt_template.template_variables\n['language', 'text']\n&gt;&gt;&gt; prompt_template.metadata['name']\n'Simple Translator'\n</code></pre> <p>Download a chat prompt template:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n&gt;&gt;&gt; prompt_template = ChatPromptTemplate.load_from_local(\"./tests/test_data/example_prompts/code_teacher.yaml\")\n&gt;&gt;&gt; print(prompt_template)\nChatPromptTemplate(template=[{'role': 'system', 'content': 'You are a coding a...', template_variables=['concept', 'programming_language'], metadata={'name': 'Code Teacher', 'description': 'A simple ...', client_parameters={}, custom_data={}, populator='jinja2', jinja2_security_level='standard')\n&gt;&gt;&gt; prompt_template.template\n[{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what {{concept}} is in {{programming_language}}.'}]\n&gt;&gt;&gt; prompt_template.template_variables\n['concept', 'programming_language']\n&gt;&gt;&gt; prompt_template.metadata['version']\n'0.0.1'\n</code></pre> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>@classmethod\ndef load_from_local(\n    cls,\n    path: Union[str, Path],\n    populator: PopulatorType = \"jinja2\",\n    jinja2_security_level: Literal[\"strict\", \"standard\", \"relaxed\"] = \"standard\",\n    yaml_library: str = \"ruamel\",\n) -&gt; Union[\"TextPromptTemplate\", \"ChatPromptTemplate\"]:\n    \"\"\"Load a prompt template from a local YAML file.\n\n    Args:\n        path (Union[str, Path]): Path to the YAML file containing the prompt template\n        populator ([PopulatorType]): The populator type to use among Literal[\"double_brace_regex\", \"single_brace_regex\", \"jinja2\"]. Defaults to \"jinja2\".\n        jinja2_security_level (Literal[\"strict\", \"standard\", \"relaxed\"], optional): The security level for the Jinja2 populator. Defaults to \"standard\".\n        yaml_library (str, optional): The YAML library to use (\"ruamel\" or \"pyyaml\"). Defaults to \"ruamel\".\n\n    Returns:\n        Union[TextPromptTemplate, ChatPromptTemplate]: The loaded template instance\n\n    Raises:\n        FileNotFoundError: If the file doesn't exist\n        ValueError: If the file is not a .yaml/.yml file\n        ValueError: If the YAML structure is invalid\n        ValueError: If attempting to load a text template with ChatPromptTemplate or vice versa\n\n    Examples:\n        Download a text prompt template:\n        &gt;&gt;&gt; from prompt_templates import TextPromptTemplate\n        &gt;&gt;&gt; prompt_template = TextPromptTemplate.load_from_local(\"./tests/test_data/example_prompts/translate.yaml\")\n        &gt;&gt;&gt; print(prompt_template)\n        TextPromptTemplate(template='Translate the following text to {{language}}:\\\\n{{..., template_variables=['language', 'text'], metadata={'name': 'Simple Translator', 'description': 'A si..., client_parameters={}, custom_data={}, populator='jinja2', jinja2_security_level='standard')\n        &gt;&gt;&gt; prompt_template.template\n        'Translate the following text to {{language}}:\\\\n{{text}}'\n        &gt;&gt;&gt; prompt_template.template_variables\n        ['language', 'text']\n        &gt;&gt;&gt; prompt_template.metadata['name']\n        'Simple Translator'\n\n        Download a chat prompt template:\n        &gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n        &gt;&gt;&gt; prompt_template = ChatPromptTemplate.load_from_local(\"./tests/test_data/example_prompts/code_teacher.yaml\")\n        &gt;&gt;&gt; print(prompt_template)\n        ChatPromptTemplate(template=[{'role': 'system', 'content': 'You are a coding a...', template_variables=['concept', 'programming_language'], metadata={'name': 'Code Teacher', 'description': 'A simple ...', client_parameters={}, custom_data={}, populator='jinja2', jinja2_security_level='standard')\n        &gt;&gt;&gt; prompt_template.template\n        [{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what {{concept}} is in {{programming_language}}.'}]\n        &gt;&gt;&gt; prompt_template.template_variables\n        ['concept', 'programming_language']\n        &gt;&gt;&gt; prompt_template.metadata['version']\n        '0.0.1'\n\n    \"\"\"\n    path = Path(path)\n    if not path.exists():\n        raise FileNotFoundError(f\"Template file not found: {path}\")\n    if path.suffix not in VALID_PROMPT_EXTENSIONS:\n        raise ValueError(f\"Template file must be a .yaml or .yml file, got: {path}\")\n\n    yaml = create_yaml_handler(yaml_library)\n    try:\n        with open(path, \"r\") as file:\n            if yaml_library == \"ruamel\":\n                prompt_file_dic = yaml.load(file)\n            else:\n                prompt_file_dic = yaml.safe_load(file)\n    except Exception as e:\n        raise ValueError(\n            f\"Failed to parse '{path}' as a valid YAML file. \"\n            f\"Please ensure the file is properly formatted.\\n\"\n            f\"Error details: {str(e)}\"\n        ) from e\n\n    cls._validate_template_type(prompt_file_dic, str(path))\n\n    return cls._load_template_from_dict(\n        prompt_file_dic, populator=populator, jinja2_security_level=jinja2_security_level\n    )\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.BasePromptTemplate.load_from_hub","title":"load_from_hub  <code>classmethod</code>","text":"<pre><code>load_from_hub(repo_id, filename, repo_type='dataset', revision=None, populator='jinja2', jinja2_security_level='standard', yaml_library='ruamel')\n</code></pre> <p>Load a prompt template from the Hugging Face Hub.</p> <p>Downloads and loads a prompt template from a repository on the Hugging Face Hub. The template file should be a YAML file following the standardized format.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>The repository ID on Hugging Face Hub (e.g., 'username/repo')</p> required <code>filename</code> <code>str</code> <p>Name of the YAML file containing the template</p> required <code>repo_type</code> <code>str</code> <p>Type of repository. Must be one of ['dataset', 'model', 'space']. Defaults to \"dataset\"</p> <code>'dataset'</code> <code>revision</code> <code>Optional[str]</code> <p>Git revision to download from. Can be a branch name, tag, or commit hash. Defaults to None</p> <code>None</code> <code>populator</code> <code>[PopulatorType]</code> <p>The populator type to use among Literal[\"double_brace_regex\", \"single_brace_regex\", \"jinja2\"]. Defaults to \"jinja2\".</p> <code>'jinja2'</code> <code>jinja2_security_level</code> <code>Literal['strict', 'standard', 'relaxed']</code> <p>The security level for the Jinja2 populator. Defaults to \"standard\".</p> <code>'standard'</code> <code>yaml_library</code> <code>str</code> <p>The YAML library to use (\"ruamel\" or \"pyyaml\"). Defaults to \"ruamel\".</p> <code>'ruamel'</code> <p>Returns:</p> Type Description <code>Union[TextPromptTemplate, ChatPromptTemplate]</code> <p>Union[TextPromptTemplate, ChatPromptTemplate]: The loaded template instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If repo_id format is invalid</p> <code>ValueError</code> <p>If repo_type is invalid</p> <code>FileNotFoundError</code> <p>If file cannot be downloaded from Hub</p> <code>ValueError</code> <p>If the YAML structure is invalid</p> <code>ValueError</code> <p>If attempting to load a text template with ChatPromptTemplate or vice versa</p> <p>Examples:</p> <p>Download a text prompt template:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import TextPromptTemplate\n&gt;&gt;&gt; prompt_template = TextPromptTemplate.load_from_hub(\n...     repo_id=\"MoritzLaurer/example_prompts\",\n...     filename=\"translate.yaml\"\n... )\n&gt;&gt;&gt; print(prompt_template)\nTextPromptTemplate(template='Translate the following text to {{language}}:\\n{{...', template_variables=['language', 'text'], metadata={'name': 'Simple Translator', 'description': 'A si...', client_parameters={}, custom_data={}, populator='jinja2', jinja2_security_level='standard')\n&gt;&gt;&gt; prompt_template.template\n'Translate the following text to {{language}}:\\n{{text}}'\n&gt;&gt;&gt; prompt_template.template_variables\n['language', 'text']\n&gt;&gt;&gt; prompt_template.metadata['name']\n'Simple Translator'\n</code></pre> <p>Download a chat prompt template:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n&gt;&gt;&gt; prompt_template = ChatPromptTemplate.load_from_hub(\n...     repo_id=\"MoritzLaurer/example_prompts\",\n...     filename=\"code_teacher.yaml\"\n... )\n&gt;&gt;&gt; print(prompt_template)\nChatPromptTemplate(template=[{'role': 'system', 'content': 'You are a coding a...', template_variables=['concept', 'programming_language'], metadata={'name': 'Code Teacher', 'description': 'A simple ...', client_parameters={}, custom_data={}, populator='jinja2', jinja2_security_level='standard')\n&gt;&gt;&gt; prompt_template.template\n[{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what {{concept}} is in {{programming_language}}.'}]\n&gt;&gt;&gt; prompt_template.template_variables\n['concept', 'programming_language']\n&gt;&gt;&gt; prompt_template.metadata['version']\n'0.0.1'\n</code></pre> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>@classmethod\ndef load_from_hub(\n    cls,\n    repo_id: str,\n    filename: str,\n    repo_type: str = \"dataset\",\n    revision: Optional[str] = None,\n    populator: PopulatorType = \"jinja2\",\n    jinja2_security_level: Literal[\"strict\", \"standard\", \"relaxed\"] = \"standard\",\n    yaml_library: str = \"ruamel\",\n) -&gt; Union[\"TextPromptTemplate\", \"ChatPromptTemplate\"]:\n    \"\"\"Load a prompt template from the Hugging Face Hub.\n\n    Downloads and loads a prompt template from a repository on the Hugging Face Hub.\n    The template file should be a YAML file following the standardized format.\n\n    Args:\n        repo_id (str): The repository ID on Hugging Face Hub (e.g., 'username/repo')\n        filename (str): Name of the YAML file containing the template\n        repo_type (str, optional): Type of repository. Must be one of\n            ['dataset', 'model', 'space']. Defaults to \"dataset\"\n        revision (Optional[str], optional): Git revision to download from.\n            Can be a branch name, tag, or commit hash. Defaults to None\n        populator ([PopulatorType]): The populator type to use among Literal[\"double_brace_regex\", \"single_brace_regex\", \"jinja2\"]. Defaults to \"jinja2\".\n        jinja2_security_level (Literal[\"strict\", \"standard\", \"relaxed\"], optional): The security level for the Jinja2 populator. Defaults to \"standard\".\n        yaml_library (str, optional): The YAML library to use (\"ruamel\" or \"pyyaml\"). Defaults to \"ruamel\".\n\n\n    Returns:\n        Union[TextPromptTemplate, ChatPromptTemplate]: The loaded template instance\n\n    Raises:\n        ValueError: If repo_id format is invalid\n        ValueError: If repo_type is invalid\n        FileNotFoundError: If file cannot be downloaded from Hub\n        ValueError: If the YAML structure is invalid\n        ValueError: If attempting to load a text template with ChatPromptTemplate or vice versa\n\n    Examples:\n        Download a text prompt template:\n        &gt;&gt;&gt; from prompt_templates import TextPromptTemplate\n        &gt;&gt;&gt; prompt_template = TextPromptTemplate.load_from_hub(\n        ...     repo_id=\"MoritzLaurer/example_prompts\",\n        ...     filename=\"translate.yaml\"\n        ... )\n        &gt;&gt;&gt; print(prompt_template)\n        TextPromptTemplate(template='Translate the following text to {{language}}:\\\\n{{...', template_variables=['language', 'text'], metadata={'name': 'Simple Translator', 'description': 'A si...', client_parameters={}, custom_data={}, populator='jinja2', jinja2_security_level='standard')\n        &gt;&gt;&gt; prompt_template.template\n        'Translate the following text to {{language}}:\\\\n{{text}}'\n        &gt;&gt;&gt; prompt_template.template_variables\n        ['language', 'text']\n        &gt;&gt;&gt; prompt_template.metadata['name']\n        'Simple Translator'\n\n        Download a chat prompt template:\n        &gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n        &gt;&gt;&gt; prompt_template = ChatPromptTemplate.load_from_hub(\n        ...     repo_id=\"MoritzLaurer/example_prompts\",\n        ...     filename=\"code_teacher.yaml\"\n        ... )\n        &gt;&gt;&gt; print(prompt_template)\n        ChatPromptTemplate(template=[{'role': 'system', 'content': 'You are a coding a...', template_variables=['concept', 'programming_language'], metadata={'name': 'Code Teacher', 'description': 'A simple ...', client_parameters={}, custom_data={}, populator='jinja2', jinja2_security_level='standard')\n        &gt;&gt;&gt; prompt_template.template\n        [{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what {{concept}} is in {{programming_language}}.'}]\n        &gt;&gt;&gt; prompt_template.template_variables\n        ['concept', 'programming_language']\n        &gt;&gt;&gt; prompt_template.metadata['version']\n        '0.0.1'\n    \"\"\"\n    # Validate Hub parameters\n    try:\n        validate_repo_id(repo_id)\n    except ValueError as e:\n        raise ValueError(f\"Invalid repo_id format: {str(e)}\") from e\n\n    if repo_type not in [\"dataset\", \"model\", \"space\"]:\n        raise ValueError(f\"repo_type must be one of ['dataset', 'model', 'space'], got {repo_type}\")\n\n    # Ensure .yaml extension\n    if not filename.endswith(VALID_PROMPT_EXTENSIONS):\n        filename += \".yaml\"\n\n    try:\n        file_path = hf_hub_download(repo_id=repo_id, filename=filename, repo_type=repo_type, revision=revision)\n    except Exception as e:\n        raise FileNotFoundError(f\"Failed to download template from Hub: {str(e)}\") from e\n\n    yaml = create_yaml_handler(yaml_library)\n    try:\n        with open(file_path, \"r\") as file:\n            if yaml_library == \"ruamel\":\n                prompt_file_dic = yaml.load(file)\n            else:\n                prompt_file_dic = yaml.safe_load(file)\n    except Exception as e:\n        raise ValueError(\n            f\"Failed to parse '{filename}' as a valid YAML file. \"\n            f\"Please ensure the file is properly formatted.\\n\"\n            f\"Error details: {str(e)}\"\n        ) from e\n\n    file_info = f\"'{filename}' from '{repo_id}'\"\n    cls._validate_template_type(prompt_file_dic, file_info)\n\n    return cls._load_template_from_dict(\n        prompt_file_dic, populator=populator, jinja2_security_level=jinja2_security_level\n    )\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.BasePromptTemplate.save_to_local","title":"save_to_local","text":"<pre><code>save_to_local(path, format=None, yaml_library='ruamel', prettify_template=True)\n</code></pre> <p>Save the prompt template as a local YAML or JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path where to save the file. Can be string or Path object</p> required <code>format</code> <code>Optional[Literal['yaml', 'json']]</code> <p>Output format (\"yaml\" or \"json\"). If None, inferred from filename</p> <code>None</code> <code>yaml_library</code> <code>str</code> <p>YAML library to use (\"ruamel\" or \"pyyaml\"). Defaults to \"ruamel\" for better formatting and format preservation.</p> <code>'ruamel'</code> <code>prettify_template</code> <code>bool</code> <p>If true format the template content with literal block scalars, i.e. \"|-\" in yaml. This makes the string behave like a Python '''...''' block to make strings easier to read and edit. Defaults to True</p> <code>True</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n&gt;&gt;&gt; messages_template = [\n...     {\"role\": \"system\", \"content\": \"You are a coding assistant who explains concepts clearly and provides short examples.\"},\n...     {\"role\": \"user\", \"content\": \"Explain what {{concept}} is in {{programming_language}}.\"}\n... ]\n&gt;&gt;&gt; template_variables = [\"concept\", \"programming_language\"]\n&gt;&gt;&gt; metadata = {\n...     \"name\": \"Code Teacher\",\n...     \"description\": \"A simple chat prompt for explaining programming concepts with examples\",\n...     \"tags\": [\"programming\", \"education\"],\n...     \"version\": \"0.0.1\",\n...     \"author\": \"My Awesome Company\"\n... }\n&gt;&gt;&gt; prompt_template = ChatPromptTemplate(\n...     template=messages_template,\n...     template_variables=template_variables,\n...     metadata=metadata,\n... )\n&gt;&gt;&gt; prompt_template.save_to_local(\"./tests/test_data/example_prompts/code_teacher_test.yaml\")\n</code></pre> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>def save_to_local(\n    self,\n    path: Union[str, Path],\n    format: Optional[Literal[\"yaml\", \"json\"]] = None,\n    yaml_library: str = \"ruamel\",\n    prettify_template: bool = True,\n) -&gt; None:\n    \"\"\"Save the prompt template as a local YAML or JSON file.\n\n    Args:\n        path: Path where to save the file. Can be string or Path object\n        format: Output format (\"yaml\" or \"json\"). If None, inferred from filename\n        yaml_library: YAML library to use (\"ruamel\" or \"pyyaml\"). Defaults to \"ruamel\" for better formatting and format preservation.\n        prettify_template: If true format the template content with literal block scalars, i.e. \"|-\" in yaml.\n            This makes the string behave like a Python '''...''' block to make strings easier to read and edit.\n            Defaults to True\n\n    Examples:\n        &gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n        &gt;&gt;&gt; messages_template = [\n        ...     {\"role\": \"system\", \"content\": \"You are a coding assistant who explains concepts clearly and provides short examples.\"},\n        ...     {\"role\": \"user\", \"content\": \"Explain what {{concept}} is in {{programming_language}}.\"}\n        ... ]\n        &gt;&gt;&gt; template_variables = [\"concept\", \"programming_language\"]\n        &gt;&gt;&gt; metadata = {\n        ...     \"name\": \"Code Teacher\",\n        ...     \"description\": \"A simple chat prompt for explaining programming concepts with examples\",\n        ...     \"tags\": [\"programming\", \"education\"],\n        ...     \"version\": \"0.0.1\",\n        ...     \"author\": \"My Awesome Company\"\n        ... }\n        &gt;&gt;&gt; prompt_template = ChatPromptTemplate(\n        ...     template=messages_template,\n        ...     template_variables=template_variables,\n        ...     metadata=metadata,\n        ... )\n        &gt;&gt;&gt; prompt_template.save_to_local(\"./tests/test_data/example_prompts/code_teacher_test.yaml\")  # doctest: +SKIP\n    \"\"\"\n\n    path = Path(path)\n    # Handle format inference and validation\n    file_extension = path.suffix.lstrip(\".\")\n    if format is None:\n        # Infer format from extension\n        if file_extension in [\"yaml\", \"yml\"]:\n            format = \"yaml\"\n        elif file_extension == \"json\":\n            format = \"json\"\n        else:\n            raise ValueError(f\"Cannot infer format from file extension: {path.suffix}\")\n    else:\n        # Validate explicitly provided format matches file extension\n        if format not in [\"yaml\", \"yml\", \"json\"]:\n            raise ValueError(f\"Unsupported format: {format}\")\n        if format in [\"yaml\", \"yml\"] and file_extension in [\"yaml\", \"yml\"]:\n            # Both are YAML variants, so they match\n            pass\n        elif format != file_extension:\n            raise ValueError(f\"Provided format '{format}' does not match file extension '{path.suffix}'\")\n\n    data = {\n        \"prompt\": {\n            \"template\": self.template,\n            \"template_variables\": self.template_variables,\n            \"metadata\": self.metadata,\n            \"client_parameters\": self.client_parameters,\n            \"custom_data\": self.custom_data,\n        }\n    }\n\n    if prettify_template:\n        data[\"prompt\"][\"template\"] = format_template_content(data[\"prompt\"][\"template\"])\n\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        if format == \"json\":\n            json.dump(data, f, indent=2, ensure_ascii=False)\n        else:  # yaml\n            yaml_handler = create_yaml_handler(yaml_library)\n            if yaml_library == \"ruamel\":\n                yaml_handler.dump(data, f)\n            elif yaml_library == \"pyyaml\":\n                yaml_handler.dump(data, f, sort_keys=False, allow_unicode=True)\n            else:\n                raise ValueError(\n                    f\"Unknown yaml library: {yaml_library}. Valid options are: 'ruamel' (default) or 'pyyaml'.\"\n                )\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.BasePromptTemplate.save_to_hub","title":"save_to_hub","text":"<pre><code>save_to_hub(repo_id, filename, repo_type='dataset', format=None, yaml_library='ruamel', prettify_template=True, token=None, create_repo=False, private=False, resource_group_id=None, revision=None, create_pr=False, commit_message=None, commit_description=None, parent_commit=None)\n</code></pre> <p>Save the prompt template to the Hugging Face Hub as a YAML or JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>The repository ID on the Hugging Face Hub (e.g., \"username/repo-name\")</p> required <code>filename</code> <code>str</code> <p>Name of the file to save (e.g., \"prompt.yaml\" or \"prompt.json\")</p> required <code>repo_type</code> <code>str</code> <p>Type of repository (\"dataset\", \"model\", or \"space\"). Defaults to \"dataset\"</p> <code>'dataset'</code> <code>token</code> <code>Optional[str]</code> <p>Hugging Face API token. If None, will use token from environment</p> <code>None</code> <code>commit_message</code> <code>Optional[str]</code> <p>Custom commit message. If None, uses default message</p> <code>None</code> <code>create_repo</code> <code>bool</code> <p>Whether to create the repository if it doesn't exist. Defaults to False</p> <code>False</code> <code>format</code> <code>Optional[Literal['yaml', 'json']]</code> <p>Output format (\"yaml\" or \"json\"). If None, inferred from filename extension</p> <code>None</code> <code>yaml_library</code> <code>str</code> <p>YAML library to use (\"ruamel\" or \"pyyaml\"). Defaults to \"ruamel\" for better formatting and format preservation.</p> <code>'ruamel'</code> <code>prettify_template</code> <code>bool</code> <p>If true format the template content with literal block scalars, i.e. \"|-\" in yaml. This makes the string behave like a Python '''...''' block to make strings easier to read and edit. Defaults to True</p> <code>True</code> <code>private</code> <code>bool</code> <p>Whether to create a private repository. Defaults to False</p> <code>False</code> <code>resource_group_id</code> <code>Optional[str]</code> <p>Optional resource group ID to associate with the repository</p> <code>None</code> <code>revision</code> <code>Optional[str]</code> <p>Optional branch/revision to push to. Defaults to main branch</p> <code>None</code> <code>create_pr</code> <code>bool</code> <p>Whether to create a Pull Request instead of pushing directly. Defaults to False</p> <code>False</code> <code>commit_description</code> <code>Optional[str]</code> <p>Optional commit description</p> <code>None</code> <code>parent_commit</code> <code>Optional[str]</code> <p>Optional parent commit to create PR from</p> <code>None</code> <p>Returns:</p> Name Type Description <code>CommitInfo</code> <code>CommitInfo</code> <p>Information about the commit/PR</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n&gt;&gt;&gt; messages_template = [\n...     {\"role\": \"system\", \"content\": \"You are a coding assistant who explains concepts clearly and provides short examples.\"},\n...     {\"role\": \"user\", \"content\": \"Explain what {{concept}} is in {{programming_language}}.\"}\n... ]\n&gt;&gt;&gt; template_variables = [\"concept\", \"programming_language\"]\n&gt;&gt;&gt; metadata = {\n...     \"name\": \"Code Teacher\",\n...     \"description\": \"A simple chat prompt for explaining programming concepts with examples\",\n...     \"tags\": [\"programming\", \"education\"],\n...     \"version\": \"0.0.1\",\n...     \"author\": \"My Awesome Company\"\n... }\n&gt;&gt;&gt; prompt_template = ChatPromptTemplate(\n...     template=messages_template,\n...     template_variables=template_variables,\n...     metadata=metadata,\n... )\n&gt;&gt;&gt; prompt_template.save_to_hub(\n...     repo_id=\"MoritzLaurer/example_prompts_test\",\n...     filename=\"code_teacher_test.yaml\",\n...     #create_repo=True,  # if the repo does not exist, create it\n...     #private=True,  # if you want to create a private repo\n...     #token=\"hf_...\"\n... )\n</code></pre> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>def save_to_hub(\n    self,\n    repo_id: str,\n    filename: str,\n    repo_type: str = \"dataset\",\n    format: Optional[Literal[\"yaml\", \"json\"]] = None,\n    yaml_library: str = \"ruamel\",\n    prettify_template: bool = True,\n    token: Optional[str] = None,\n    create_repo: bool = False,\n    private: bool = False,\n    resource_group_id: Optional[str] = None,\n    revision: Optional[str] = None,\n    create_pr: bool = False,\n    commit_message: Optional[str] = None,\n    commit_description: Optional[str] = None,\n    parent_commit: Optional[str] = None,\n) -&gt; CommitInfo:\n    \"\"\"Save the prompt template to the Hugging Face Hub as a YAML or JSON file.\n\n    Args:\n        repo_id: The repository ID on the Hugging Face Hub (e.g., \"username/repo-name\")\n        filename: Name of the file to save (e.g., \"prompt.yaml\" or \"prompt.json\")\n        repo_type: Type of repository (\"dataset\", \"model\", or \"space\"). Defaults to \"dataset\"\n        token: Hugging Face API token. If None, will use token from environment\n        commit_message: Custom commit message. If None, uses default message\n        create_repo: Whether to create the repository if it doesn't exist. Defaults to False\n        format: Output format (\"yaml\" or \"json\"). If None, inferred from filename extension\n        yaml_library: YAML library to use (\"ruamel\" or \"pyyaml\"). Defaults to \"ruamel\" for better formatting and format preservation.\n        prettify_template: If true format the template content with literal block scalars, i.e. \"|-\" in yaml.\n            This makes the string behave like a Python '''...''' block to make strings easier to read and edit.\n            Defaults to True\n        private: Whether to create a private repository. Defaults to False\n        resource_group_id: Optional resource group ID to associate with the repository\n        revision: Optional branch/revision to push to. Defaults to main branch\n        create_pr: Whether to create a Pull Request instead of pushing directly. Defaults to False\n        commit_description: Optional commit description\n        parent_commit: Optional parent commit to create PR from\n\n    Returns:\n        CommitInfo: Information about the commit/PR\n\n    Examples:\n        &gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n        &gt;&gt;&gt; messages_template = [\n        ...     {\"role\": \"system\", \"content\": \"You are a coding assistant who explains concepts clearly and provides short examples.\"},\n        ...     {\"role\": \"user\", \"content\": \"Explain what {{concept}} is in {{programming_language}}.\"}\n        ... ]\n        &gt;&gt;&gt; template_variables = [\"concept\", \"programming_language\"]\n        &gt;&gt;&gt; metadata = {\n        ...     \"name\": \"Code Teacher\",\n        ...     \"description\": \"A simple chat prompt for explaining programming concepts with examples\",\n        ...     \"tags\": [\"programming\", \"education\"],\n        ...     \"version\": \"0.0.1\",\n        ...     \"author\": \"My Awesome Company\"\n        ... }\n        &gt;&gt;&gt; prompt_template = ChatPromptTemplate(\n        ...     template=messages_template,\n        ...     template_variables=template_variables,\n        ...     metadata=metadata,\n        ... )\n        &gt;&gt;&gt; prompt_template.save_to_hub(  # doctest: +SKIP\n        ...     repo_id=\"MoritzLaurer/example_prompts_test\",\n        ...     filename=\"code_teacher_test.yaml\",\n        ...     #create_repo=True,  # if the repo does not exist, create it\n        ...     #private=True,  # if you want to create a private repo\n        ...     #token=\"hf_...\"\n        ... )\n    \"\"\"\n\n    # Handle format inference and validation\n    if format is None:\n        # Infer format from extension\n        extension = Path(filename).suffix.lstrip(\".\")\n        if extension in [\"yaml\", \"yml\"]:\n            format = \"yaml\"\n        elif extension == \"json\":\n            format = \"json\"\n        else:\n            format = \"yaml\"  # default if no extension\n            filename += \".yaml\"\n    else:\n        # Validate explicitly provided format matches file extension\n        if format not in [\"yaml\", \"yml\", \"json\"]:\n            raise ValueError(f\"Unsupported format: {format}\")\n\n        file_extension = Path(filename).suffix.lstrip(\".\")\n        if format in [\"yaml\", \"yml\"] and file_extension in [\"yaml\", \"yml\"]:\n            # Both are YAML variants, so they match\n            pass\n        elif format != file_extension:\n            raise ValueError(f\"Provided format '{format}' does not match file extension '{filename}'\")\n\n    # Convert template to the specified format\n    data = {\n        \"prompt\": {\n            \"template\": self.template,\n            \"template_variables\": self.template_variables,\n            \"metadata\": self.metadata,\n            \"client_parameters\": self.client_parameters,\n            \"custom_data\": self.custom_data,\n        }\n    }\n\n    if prettify_template:\n        data[\"prompt\"][\"template\"] = format_template_content(data[\"prompt\"][\"template\"])\n\n    if format == \"json\":\n        content = json.dumps(data, indent=2, ensure_ascii=False)\n        content_bytes = content.encode(\"utf-8\")\n    else:  # yaml\n        yaml_handler = create_yaml_handler(yaml_library)\n        string_stream = io.StringIO()\n        yaml_handler.dump(data, string_stream)\n        content = string_stream.getvalue()\n        content_bytes = content.encode(\"utf-8\")\n\n    # Upload to Hub\n    api = HfApi(token=token)\n\n    # Check if repo exists before attempting to create it to avoid overwriting repo card\n    try:\n        api.repo_info(repo_id=repo_id, repo_type=repo_type)\n        repo_exists = True\n    except RepositoryNotFoundError:\n        repo_exists = False\n\n    if create_repo and repo_exists:\n        logger.info(\n            f\"You specified create_repo={create_repo}, but repository {repo_id} already exists. \"\n            \"Skipping repo creation.\"\n        )\n    elif not create_repo and not repo_exists:\n        raise ValueError(f\"Repository {repo_id} does not exist. Set create_repo=True to create it.\")\n    elif create_repo and not repo_exists:\n        logger.info(f\"Creating/Updating HF Hub repository {repo_id}\")\n        api.create_repo(\n            repo_id=repo_id,\n            repo_type=repo_type,\n            token=token,\n            private=private,\n            # exist_ok=exist_ok,  # not using this arg to avoid inconsistency\n            resource_group_id=resource_group_id,\n        )\n        repocard_text = (\n            \"---\\n\"\n            \"library_name: prompt-templates\\n\"\n            \"tags:\\n\"\n            \"- prompts\\n\"\n            \"- prompt-templates\\n\"\n            \"---\\n\"\n            \"This repository was created with the `prompt-templates` library and contains\\n\"\n            \"prompt templates in the `Files` tab.\\n\"\n            \"For easily reusing these templates, see the [documentation](https://github.com/MoritzLaurer/prompt-templates).\"\n        )\n        card = RepoCard(repocard_text)\n        card.push_to_hub(\n            repo_id=repo_id,\n            repo_type=repo_type,\n            token=token,\n            commit_message=\"Create/Update repo card with prompt-templates library\",\n            create_pr=create_pr,\n            parent_commit=parent_commit,\n        )\n    elif not create_repo and repo_exists:\n        # Update repo metadata to make prompt templates discoverable on the HF Hub\n        logger.info(f\"Updating HF Hub repository {repo_id} with prompt-templates library metadata.\")\n        metadata_update(\n            repo_id=repo_id,\n            metadata={\"library_name\": \"prompt-templates\", \"tags\": [\"prompts\", \"prompt-templates\"]},\n            repo_type=repo_type,\n            overwrite=False,\n            token=token,\n            commit_message=commit_message or \"Update repo metadata with prompt-templates library\",\n            commit_description=commit_description,\n            revision=revision,\n            create_pr=create_pr,\n            parent_commit=parent_commit,\n        )\n\n    # Upload file\n    logger.info(f\"Uploading prompt template {filename} to HF Hub repository {repo_id}\")\n    return api.upload_file(\n        path_or_fileobj=io.BytesIO(content_bytes),\n        path_in_repo=filename,\n        repo_id=repo_id,\n        repo_type=repo_type,\n        token=token,\n        commit_message=commit_message or f\"Upload prompt template {filename}\",\n        commit_description=commit_description,\n        revision=revision,\n        create_pr=create_pr,\n        parent_commit=parent_commit,\n    )\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.BasePromptTemplate.display","title":"display","text":"<pre><code>display(format='json')\n</code></pre> <p>Display the prompt configuration in the specified format.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import TextPromptTemplate\n&gt;&gt;&gt; prompt_template = TextPromptTemplate.load_from_hub(\n...     repo_id=\"MoritzLaurer/example_prompts\",\n...     filename=\"translate.yaml\"\n... )\n&gt;&gt;&gt; prompt_template.display(format=\"yaml\")\ntemplate: |-\n  Translate the following text to {{language}}:\n  {{text}}\ntemplate_variables:\n- language\n- text\nmetadata:\n  name: \"Simple Translator\"\n  description: \"A simple translation prompt for illustrating the standard prompt YAML\n    format\"\n  tags:\n  - translation\n  - multilinguality\n  version: \"0.0.1\"\n  author: \"Guy van Babel\"\nclient_parameters: {}\ncustom_data: {}\n</code></pre> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>def display(self, format: Literal[\"json\", \"yaml\"] = \"json\") -&gt; None:\n    \"\"\"Display the prompt configuration in the specified format.\n\n    Examples:\n        &gt;&gt;&gt; from prompt_templates import TextPromptTemplate\n        &gt;&gt;&gt; prompt_template = TextPromptTemplate.load_from_hub(\n        ...     repo_id=\"MoritzLaurer/example_prompts\",\n        ...     filename=\"translate.yaml\"\n        ... )\n        &gt;&gt;&gt; prompt_template.display(format=\"yaml\")  # doctest: +NORMALIZE_WHITESPACE\n        template: |-\n          Translate the following text to {{language}}:\n          {{text}}\n        template_variables:\n        - language\n        - text\n        metadata:\n          name: \"Simple Translator\"\n          description: \"A simple translation prompt for illustrating the standard prompt YAML\n            format\"\n          tags:\n          - translation\n          - multilinguality\n          version: \"0.0.1\"\n          author: \"Guy van Babel\"\n        client_parameters: {}\n        custom_data: {}\n    \"\"\"\n    # Create a clean dict with only the relevant attributes\n    display_dict = {\n        \"template\": self.template,\n        \"template_variables\": self.template_variables,\n        \"metadata\": self.metadata,\n        \"client_parameters\": self.client_parameters,\n        \"custom_data\": self.custom_data,\n    }\n\n    if format == \"json\":\n        print(json.dumps(display_dict, indent=2), end=\"\")\n    elif format == \"yaml\":\n        # Create a new YAML instance without version/tags output\n        yaml_handler = YAML()\n        yaml_handler.explicit_start = False\n        yaml_handler.explicit_end = False\n        yaml_handler.version = None\n\n        # Dump to string first to avoid stdout formatting issues\n        output = io.StringIO()\n        yaml_handler.dump(display_dict, output)\n        print(output.getvalue(), end=\"\")\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.TextPromptTemplate","title":"TextPromptTemplate","text":"<p>             Bases: <code>BasePromptTemplate</code></p> <p>A class representing a standard text prompt template.</p> <p>Examples:</p> <p>Instantiate a text prompt template:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import TextPromptTemplate\n&gt;&gt;&gt; template_text = \"Translate the following text to {{language}}:\\n{{text}}\"\n&gt;&gt;&gt; template_variables = [\"language\", \"text\"]\n&gt;&gt;&gt; metadata = {\n...     \"name\": \"Simple Translator\",\n...     \"description\": \"A simple translation prompt for illustrating the standard prompt YAML format\",\n...     \"tags\": [\"translation\", \"multilinguality\"],\n...     \"version\": \"0.0.1\",\n...     \"author\": \"Guy van Babel\"\n... }\n&gt;&gt;&gt; prompt_template = TextPromptTemplate(\n...     template=template_text,\n...     template_variables=template_variables,\n...     metadata=metadata\n... )\n&gt;&gt;&gt; print(prompt_template)\nTextPromptTemplate(template='Translate the following text to {{language}}:\\n{{..., template_variables=['language', 'text'], metadata={'name': 'Simple Translator', 'description': 'A si..., client_parameters={}, custom_data={}, populator='jinja2', jinja2_security_level='standard')\n</code></pre> <pre><code>&gt;&gt;&gt; # Inspect template attributes\n&gt;&gt;&gt; prompt_template.template\n'Translate the following text to {{language}}:\\n{{text}}'\n&gt;&gt;&gt; prompt_template.template_variables\n['language', 'text']\n&gt;&gt;&gt; prompt_template.metadata['name']\n'Simple Translator'\n</code></pre> <pre><code>&gt;&gt;&gt; # Populate the template\n&gt;&gt;&gt; prompt = prompt_template.populate(\n...     language=\"French\",\n...     text=\"Hello world!\"\n... )\n&gt;&gt;&gt; print(prompt)\nTranslate the following text to French:\nHello world!\n</code></pre> <p>Or download the same text prompt template from the Hub:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import TextPromptTemplate\n&gt;&gt;&gt; prompt_template_downloaded = TextPromptTemplate.load_from_hub(\n...     repo_id=\"MoritzLaurer/example_prompts\",\n...     filename=\"translate.yaml\"\n... )\n&gt;&gt;&gt; prompt_template_downloaded == prompt_template\nTrue\n</code></pre> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>class TextPromptTemplate(BasePromptTemplate):\n    \"\"\"A class representing a standard text prompt template.\n\n    Examples:\n        Instantiate a text prompt template:\n        &gt;&gt;&gt; from prompt_templates import TextPromptTemplate\n        &gt;&gt;&gt; template_text = \"Translate the following text to {{language}}:\\\\n{{text}}\"\n        &gt;&gt;&gt; template_variables = [\"language\", \"text\"]\n        &gt;&gt;&gt; metadata = {\n        ...     \"name\": \"Simple Translator\",\n        ...     \"description\": \"A simple translation prompt for illustrating the standard prompt YAML format\",\n        ...     \"tags\": [\"translation\", \"multilinguality\"],\n        ...     \"version\": \"0.0.1\",\n        ...     \"author\": \"Guy van Babel\"\n        ... }\n        &gt;&gt;&gt; prompt_template = TextPromptTemplate(\n        ...     template=template_text,\n        ...     template_variables=template_variables,\n        ...     metadata=metadata\n        ... )\n        &gt;&gt;&gt; print(prompt_template)\n        TextPromptTemplate(template='Translate the following text to {{language}}:\\\\n{{..., template_variables=['language', 'text'], metadata={'name': 'Simple Translator', 'description': 'A si..., client_parameters={}, custom_data={}, populator='jinja2', jinja2_security_level='standard')\n\n        &gt;&gt;&gt; # Inspect template attributes\n        &gt;&gt;&gt; prompt_template.template\n        'Translate the following text to {{language}}:\\\\n{{text}}'\n        &gt;&gt;&gt; prompt_template.template_variables\n        ['language', 'text']\n        &gt;&gt;&gt; prompt_template.metadata['name']\n        'Simple Translator'\n\n        &gt;&gt;&gt; # Populate the template\n        &gt;&gt;&gt; prompt = prompt_template.populate(\n        ...     language=\"French\",\n        ...     text=\"Hello world!\"\n        ... )\n        &gt;&gt;&gt; print(prompt)\n        Translate the following text to French:\n        Hello world!\n\n        Or download the same text prompt template from the Hub:\n        &gt;&gt;&gt; from prompt_templates import TextPromptTemplate\n        &gt;&gt;&gt; prompt_template_downloaded = TextPromptTemplate.load_from_hub(\n        ...     repo_id=\"MoritzLaurer/example_prompts\",\n        ...     filename=\"translate.yaml\"\n        ... )\n        &gt;&gt;&gt; prompt_template_downloaded == prompt_template\n        True\n    \"\"\"\n\n    def __init__(\n        self,\n        template: str,\n        template_variables: Optional[List[str]] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        client_parameters: Optional[Dict[str, Any]] = None,\n        custom_data: Optional[Dict[str, Any]] = None,\n        populator: PopulatorType = \"jinja2\",\n        jinja2_security_level: Jinja2SecurityLevel = \"standard\",\n    ) -&gt; None:\n        super().__init__(\n            template=template,\n            template_variables=template_variables,\n            metadata=metadata,\n            client_parameters=client_parameters,\n            custom_data=custom_data,\n            populator=populator,\n            jinja2_security_level=jinja2_security_level,\n        )\n\n    def populate(self, **user_provided_variables: Any) -&gt; str:\n        \"\"\"Populate the prompt by replacing placeholders with provided values.\n\n        Examples:\n            &gt;&gt;&gt; from prompt_templates import TextPromptTemplate\n            &gt;&gt;&gt; prompt_template = TextPromptTemplate.load_from_hub(\n            ...     repo_id=\"MoritzLaurer/example_prompts\",\n            ...     filename=\"translate.yaml\"\n            ... )\n            &gt;&gt;&gt; prompt_template.template\n            'Translate the following text to {{language}}:\\\\n{{text}}'\n            &gt;&gt;&gt; prompt = prompt_template.populate(\n            ...     language=\"French\",\n            ...     text=\"Hello world!\"\n            ... )\n            &gt;&gt;&gt; print(prompt)\n            Translate the following text to French:\n            Hello world!\n\n        Args:\n            **user_provided_variables: The values to fill placeholders in the prompt template.\n\n        Returns:\n            str: The populated prompt string.\n        \"\"\"\n        self._validate_user_provided_variables(user_provided_variables)\n        populated_prompt = str(self._populate_placeholders(self.template, user_provided_variables))\n        return populated_prompt\n\n    def to_langchain_template(self) -&gt; \"LC_PromptTemplate\":\n        \"\"\"Convert the TextPromptTemplate to a LangChain PromptTemplate.\n\n        Examples:\n            &gt;&gt;&gt; from prompt_templates import TextPromptTemplate\n            &gt;&gt;&gt; prompt_template = TextPromptTemplate.load_from_hub(\n            ...     repo_id=\"MoritzLaurer/example_prompts\",\n            ...     filename=\"translate.yaml\"\n            ... )\n            &gt;&gt;&gt; lc_template = prompt_template.to_langchain_template()\n            &gt;&gt;&gt; # test equivalence\n            &gt;&gt;&gt; from langchain_core.prompts import PromptTemplate as LC_PromptTemplate\n            &gt;&gt;&gt; isinstance(lc_template, LC_PromptTemplate)\n            True\n\n        Returns:\n            PromptTemplate: A LangChain PromptTemplate object.\n\n        Raises:\n            ImportError: If LangChain is not installed.\n        \"\"\"\n        try:\n            from langchain_core.prompts import PromptTemplate as LC_PromptTemplate\n        except ImportError as e:\n            raise ImportError(\"LangChain is not installed. Please install it with 'pip install langchain'\") from e\n\n        return LC_PromptTemplate(\n            template=self.template,\n            input_variables=self.template_variables,\n            metadata=self.metadata,\n        )\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.TextPromptTemplate.populate","title":"populate","text":"<pre><code>populate(**user_provided_variables)\n</code></pre> <p>Populate the prompt by replacing placeholders with provided values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import TextPromptTemplate\n&gt;&gt;&gt; prompt_template = TextPromptTemplate.load_from_hub(\n...     repo_id=\"MoritzLaurer/example_prompts\",\n...     filename=\"translate.yaml\"\n... )\n&gt;&gt;&gt; prompt_template.template\n'Translate the following text to {{language}}:\\n{{text}}'\n&gt;&gt;&gt; prompt = prompt_template.populate(\n...     language=\"French\",\n...     text=\"Hello world!\"\n... )\n&gt;&gt;&gt; print(prompt)\nTranslate the following text to French:\nHello world!\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>**user_provided_variables</code> <code>Any</code> <p>The values to fill placeholders in the prompt template.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The populated prompt string.</p> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>def populate(self, **user_provided_variables: Any) -&gt; str:\n    \"\"\"Populate the prompt by replacing placeholders with provided values.\n\n    Examples:\n        &gt;&gt;&gt; from prompt_templates import TextPromptTemplate\n        &gt;&gt;&gt; prompt_template = TextPromptTemplate.load_from_hub(\n        ...     repo_id=\"MoritzLaurer/example_prompts\",\n        ...     filename=\"translate.yaml\"\n        ... )\n        &gt;&gt;&gt; prompt_template.template\n        'Translate the following text to {{language}}:\\\\n{{text}}'\n        &gt;&gt;&gt; prompt = prompt_template.populate(\n        ...     language=\"French\",\n        ...     text=\"Hello world!\"\n        ... )\n        &gt;&gt;&gt; print(prompt)\n        Translate the following text to French:\n        Hello world!\n\n    Args:\n        **user_provided_variables: The values to fill placeholders in the prompt template.\n\n    Returns:\n        str: The populated prompt string.\n    \"\"\"\n    self._validate_user_provided_variables(user_provided_variables)\n    populated_prompt = str(self._populate_placeholders(self.template, user_provided_variables))\n    return populated_prompt\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.TextPromptTemplate.to_langchain_template","title":"to_langchain_template","text":"<pre><code>to_langchain_template()\n</code></pre> <p>Convert the TextPromptTemplate to a LangChain PromptTemplate.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import TextPromptTemplate\n&gt;&gt;&gt; prompt_template = TextPromptTemplate.load_from_hub(\n...     repo_id=\"MoritzLaurer/example_prompts\",\n...     filename=\"translate.yaml\"\n... )\n&gt;&gt;&gt; lc_template = prompt_template.to_langchain_template()\n&gt;&gt;&gt; # test equivalence\n&gt;&gt;&gt; from langchain_core.prompts import PromptTemplate as LC_PromptTemplate\n&gt;&gt;&gt; isinstance(lc_template, LC_PromptTemplate)\nTrue\n</code></pre> <p>Returns:</p> Name Type Description <code>PromptTemplate</code> <code>PromptTemplate</code> <p>A LangChain PromptTemplate object.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If LangChain is not installed.</p> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>def to_langchain_template(self) -&gt; \"LC_PromptTemplate\":\n    \"\"\"Convert the TextPromptTemplate to a LangChain PromptTemplate.\n\n    Examples:\n        &gt;&gt;&gt; from prompt_templates import TextPromptTemplate\n        &gt;&gt;&gt; prompt_template = TextPromptTemplate.load_from_hub(\n        ...     repo_id=\"MoritzLaurer/example_prompts\",\n        ...     filename=\"translate.yaml\"\n        ... )\n        &gt;&gt;&gt; lc_template = prompt_template.to_langchain_template()\n        &gt;&gt;&gt; # test equivalence\n        &gt;&gt;&gt; from langchain_core.prompts import PromptTemplate as LC_PromptTemplate\n        &gt;&gt;&gt; isinstance(lc_template, LC_PromptTemplate)\n        True\n\n    Returns:\n        PromptTemplate: A LangChain PromptTemplate object.\n\n    Raises:\n        ImportError: If LangChain is not installed.\n    \"\"\"\n    try:\n        from langchain_core.prompts import PromptTemplate as LC_PromptTemplate\n    except ImportError as e:\n        raise ImportError(\"LangChain is not installed. Please install it with 'pip install langchain'\") from e\n\n    return LC_PromptTemplate(\n        template=self.template,\n        input_variables=self.template_variables,\n        metadata=self.metadata,\n    )\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.ChatPromptTemplate","title":"ChatPromptTemplate","text":"<p>             Bases: <code>BasePromptTemplate</code></p> <p>A class representing a chat prompt template that can be formatted for and used with various LLM clients.</p> <p>Examples:</p> <p>Instantiate a chat prompt template:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n&gt;&gt;&gt; template_messages = [\n...     {\"role\": \"system\", \"content\": \"You are a coding assistant who explains concepts clearly and provides short examples.\"},\n...     {\"role\": \"user\", \"content\": \"Explain what {{concept}} is in {{programming_language}}.\"}\n... ]\n&gt;&gt;&gt; template_variables = [\"concept\", \"programming_language\"]\n&gt;&gt;&gt; metadata = {\n...     \"name\": \"Code Teacher\",\n...     \"description\": \"A simple chat prompt for explaining programming concepts with examples\",\n...     \"tags\": [\"programming\", \"education\"],\n...     \"version\": \"0.0.1\",\n...     \"author\": \"Guido van Bossum\"\n... }\n&gt;&gt;&gt; prompt_template = ChatPromptTemplate(\n...     template=template_messages,\n...     template_variables=template_variables,\n...     metadata=metadata\n... )\n&gt;&gt;&gt; print(prompt_template)\nChatPromptTemplate(template=[{'role': 'system', 'content': 'You are a coding a..., template_variables=['concept', 'programming_language'], metadata={'name': 'Code Teacher', 'description': 'A simple ..., client_parameters={}, custom_data={}, populator='jinja2', jinja2_security_level='standard')\n&gt;&gt;&gt; # Inspect template attributes\n&gt;&gt;&gt; prompt_template.template\n[{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what {{concept}} is in {{programming_language}}.'}]\n&gt;&gt;&gt; prompt_template.template_variables\n['concept', 'programming_language']\n</code></pre> <pre><code>&gt;&gt;&gt; # Populate the template\n&gt;&gt;&gt; messages = prompt_template.populate(\n...     concept=\"list comprehension\",\n...     programming_language=\"Python\"\n... )\n&gt;&gt;&gt; print(messages)\n[{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]\n</code></pre> <pre><code>&gt;&gt;&gt; # By default, the populated prompt is in the OpenAI messages format, as it is adopted by many open-source libraries\n&gt;&gt;&gt; # You can convert to formats used by other LLM clients like Anthropic's or Google Gemini's like this:\n&gt;&gt;&gt; from prompt_templates import format_for_client\n&gt;&gt;&gt; messages_anthropic = format_for_client(messages, \"anthropic\")\n&gt;&gt;&gt; print(messages_anthropic)\n{'system': 'You are a coding assistant who explains concepts clearly and provides short examples.', 'messages': [{'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]}\n</code></pre> <pre><code>&gt;&gt;&gt; # Convenience method to populate and format in one step for clients that do not use the OpenAI messages format\n&gt;&gt;&gt; messages_anthropic = prompt_template.create_messages(\n...     client=\"anthropic\",\n...     concept=\"list comprehension\",\n...     programming_language=\"Python\"\n... )\n&gt;&gt;&gt; print(messages_anthropic)\n{'system': 'You are a coding assistant who explains concepts clearly and provides short examples.', 'messages': [{'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]}\n</code></pre> <p>Or download the same chat prompt template from the Hub:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n&gt;&gt;&gt; prompt_template_downloaded = ChatPromptTemplate.load_from_hub(\n...     repo_id=\"MoritzLaurer/example_prompts\",\n...     filename=\"code_teacher.yaml\"\n... )\n&gt;&gt;&gt; prompt_template_downloaded == prompt_template\nTrue\n</code></pre> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>class ChatPromptTemplate(BasePromptTemplate):\n    \"\"\"A class representing a chat prompt template that can be formatted for and used with various LLM clients.\n\n    Examples:\n        Instantiate a chat prompt template:\n        &gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n        &gt;&gt;&gt; template_messages = [\n        ...     {\"role\": \"system\", \"content\": \"You are a coding assistant who explains concepts clearly and provides short examples.\"},\n        ...     {\"role\": \"user\", \"content\": \"Explain what {{concept}} is in {{programming_language}}.\"}\n        ... ]\n        &gt;&gt;&gt; template_variables = [\"concept\", \"programming_language\"]\n        &gt;&gt;&gt; metadata = {\n        ...     \"name\": \"Code Teacher\",\n        ...     \"description\": \"A simple chat prompt for explaining programming concepts with examples\",\n        ...     \"tags\": [\"programming\", \"education\"],\n        ...     \"version\": \"0.0.1\",\n        ...     \"author\": \"Guido van Bossum\"\n        ... }\n        &gt;&gt;&gt; prompt_template = ChatPromptTemplate(\n        ...     template=template_messages,\n        ...     template_variables=template_variables,\n        ...     metadata=metadata\n        ... )\n        &gt;&gt;&gt; print(prompt_template)\n        ChatPromptTemplate(template=[{'role': 'system', 'content': 'You are a coding a..., template_variables=['concept', 'programming_language'], metadata={'name': 'Code Teacher', 'description': 'A simple ..., client_parameters={}, custom_data={}, populator='jinja2', jinja2_security_level='standard')\n        &gt;&gt;&gt; # Inspect template attributes\n        &gt;&gt;&gt; prompt_template.template\n        [{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what {{concept}} is in {{programming_language}}.'}]\n        &gt;&gt;&gt; prompt_template.template_variables\n        ['concept', 'programming_language']\n\n        &gt;&gt;&gt; # Populate the template\n        &gt;&gt;&gt; messages = prompt_template.populate(\n        ...     concept=\"list comprehension\",\n        ...     programming_language=\"Python\"\n        ... )\n        &gt;&gt;&gt; print(messages)\n        [{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]\n\n        &gt;&gt;&gt; # By default, the populated prompt is in the OpenAI messages format, as it is adopted by many open-source libraries\n        &gt;&gt;&gt; # You can convert to formats used by other LLM clients like Anthropic's or Google Gemini's like this:\n        &gt;&gt;&gt; from prompt_templates import format_for_client\n        &gt;&gt;&gt; messages_anthropic = format_for_client(messages, \"anthropic\")\n        &gt;&gt;&gt; print(messages_anthropic)\n        {'system': 'You are a coding assistant who explains concepts clearly and provides short examples.', 'messages': [{'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]}\n\n        &gt;&gt;&gt; # Convenience method to populate and format in one step for clients that do not use the OpenAI messages format\n        &gt;&gt;&gt; messages_anthropic = prompt_template.create_messages(\n        ...     client=\"anthropic\",\n        ...     concept=\"list comprehension\",\n        ...     programming_language=\"Python\"\n        ... )\n        &gt;&gt;&gt; print(messages_anthropic)\n        {'system': 'You are a coding assistant who explains concepts clearly and provides short examples.', 'messages': [{'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]}\n\n        Or download the same chat prompt template from the Hub:\n        &gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n        &gt;&gt;&gt; prompt_template_downloaded = ChatPromptTemplate.load_from_hub(\n        ...     repo_id=\"MoritzLaurer/example_prompts\",\n        ...     filename=\"code_teacher.yaml\"\n        ... )\n        &gt;&gt;&gt; prompt_template_downloaded == prompt_template\n        True\n    \"\"\"\n\n    template: List[Dict[str, str]]\n\n    def __init__(\n        self,\n        template: List[Dict[str, Any]],\n        template_variables: Optional[List[str]] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        client_parameters: Optional[Dict[str, Any]] = None,\n        custom_data: Optional[Dict[str, Any]] = None,\n        populator: PopulatorType = \"jinja2\",\n        jinja2_security_level: Jinja2SecurityLevel = \"standard\",\n    ) -&gt; None:\n        super().__init__(\n            template=template,\n            template_variables=template_variables,\n            metadata=metadata,\n            client_parameters=client_parameters,\n            custom_data=custom_data,\n            populator=populator,\n            jinja2_security_level=jinja2_security_level,\n        )\n\n    def populate(self, **user_provided_variables: Any) -&gt; List[Dict[str, Any]]:\n        \"\"\"Populate the prompt template messages by replacing placeholders with provided values.\n\n        Examples:\n            &gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n            &gt;&gt;&gt; prompt_template = ChatPromptTemplate.load_from_hub(\n            ...     repo_id=\"MoritzLaurer/example_prompts\",\n            ...     filename=\"code_teacher.yaml\"\n            ... )\n            &gt;&gt;&gt; messages = prompt_template.populate(\n            ...     concept=\"list comprehension\",\n            ...     programming_language=\"Python\"\n            ... )\n            &gt;&gt;&gt; print(messages)\n            [{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]\n\n        Args:\n            **user_provided_variables: The values to fill placeholders in the messages template.\n\n        Returns:\n            List[Dict[str, Any]]: The populated prompt as a list in the OpenAI messages format.\n        \"\"\"\n        self._validate_user_provided_variables(user_provided_variables)\n\n        messages_template_populated: List[Dict[str, str]] = [\n            {\n                \"role\": str(message[\"role\"]),\n                \"content\": self._populate_placeholders(message[\"content\"], user_provided_variables),\n            }\n            for message in self.template\n        ]\n        return messages_template_populated\n\n    def create_messages(\n        self, client: ClientType = \"openai\", **user_provided_variables: Any\n    ) -&gt; List[Dict[str, Any]] | Dict[str, Any]:\n        \"\"\"Convenience method that populates a prompt template and formats it for a client in one step.\n        This method is only useful if your a client that does not use the OpenAI messages format, because\n        populating a ChatPromptTemplate converts it into the OpenAI messages format by default.\n\n        Examples:\n            &gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n            &gt;&gt;&gt; prompt_template = ChatPromptTemplate.load_from_hub(\n            ...     repo_id=\"MoritzLaurer/example_prompts\",\n            ...     filename=\"code_teacher.yaml\"\n            ... )\n            &gt;&gt;&gt; # Format for OpenAI (default)\n            &gt;&gt;&gt; messages = prompt_template.create_messages(\n            ...     concept=\"list comprehension\",\n            ...     programming_language=\"Python\"\n            ... )\n            &gt;&gt;&gt; print(messages)\n            [{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]\n\n            &gt;&gt;&gt; # Format for Anthropic\n            &gt;&gt;&gt; messages = prompt_template.create_messages(\n            ...     client=\"anthropic\",\n            ...     concept=\"list comprehension\",\n            ...     programming_language=\"Python\"\n            ... )\n            &gt;&gt;&gt; messages\n            {'system': 'You are a coding assistant who explains concepts clearly and provides short examples.', 'messages': [{'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]}\n\n        Args:\n            client (str): The client format to use ('openai', 'anthropic', 'google'). Defaults to 'openai'.\n            **user_provided_variables: The variables to fill into the prompt template. For example, if your template\n                expects variables like 'name' and 'age', pass them as keyword arguments.\n\n        Returns:\n            List[Dict[str, Any]] | Dict[str, Any]: A populated prompt formatted for the specified client.\n        \"\"\"\n        if \"client\" in user_provided_variables:\n            logger.warning(\n                f\"'client' was passed both as a parameter for the LLM inference client ('{client}') and in user_provided_variables \"\n                f\"('{user_provided_variables['client']}'). The first parameter version will be used for formatting, \"\n                \"while the second user_provided_variable version will be used in template population.\"\n            )\n\n        prompt = self.populate(**user_provided_variables)\n        return format_for_client(prompt, client)\n\n    def to_langchain_template(self) -&gt; \"LC_ChatPromptTemplate\":\n        \"\"\"Convert the ChatPromptTemplate to a LangChain ChatPromptTemplate.\n\n        Examples:\n            &gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n            &gt;&gt;&gt; prompt_template = ChatPromptTemplate.load_from_hub(\n            ...     repo_id=\"MoritzLaurer/example_prompts\",\n            ...     filename=\"code_teacher.yaml\"\n            ... )\n            &gt;&gt;&gt; lc_template = prompt_template.to_langchain_template()\n            &gt;&gt;&gt; # test equivalence\n            &gt;&gt;&gt; from langchain_core.prompts import ChatPromptTemplate as LC_ChatPromptTemplate\n            &gt;&gt;&gt; isinstance(lc_template, LC_ChatPromptTemplate)\n            True\n\n        Returns:\n            ChatPromptTemplate: A LangChain ChatPromptTemplate object.\n\n        Raises:\n            ImportError: If LangChain is not installed.\n        \"\"\"\n        try:\n            from langchain_core.prompts import ChatPromptTemplate as LC_ChatPromptTemplate\n        except ImportError as e:\n            raise ImportError(\"LangChain is not installed. Please install it with 'pip install langchain'\") from e\n\n        # LangChain expects a list of tuples of the form (role, content)\n        messages: List[Tuple[str, str]] = [\n            (str(message[\"role\"]), str(message[\"content\"])) for message in self.template\n        ]\n        return LC_ChatPromptTemplate(\n            messages=messages,\n            input_variables=self.template_variables,\n            metadata=self.metadata,\n        )\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.ChatPromptTemplate.populate","title":"populate","text":"<pre><code>populate(**user_provided_variables)\n</code></pre> <p>Populate the prompt template messages by replacing placeholders with provided values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n&gt;&gt;&gt; prompt_template = ChatPromptTemplate.load_from_hub(\n...     repo_id=\"MoritzLaurer/example_prompts\",\n...     filename=\"code_teacher.yaml\"\n... )\n&gt;&gt;&gt; messages = prompt_template.populate(\n...     concept=\"list comprehension\",\n...     programming_language=\"Python\"\n... )\n&gt;&gt;&gt; print(messages)\n[{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>**user_provided_variables</code> <code>Any</code> <p>The values to fill placeholders in the messages template.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: The populated prompt as a list in the OpenAI messages format.</p> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>def populate(self, **user_provided_variables: Any) -&gt; List[Dict[str, Any]]:\n    \"\"\"Populate the prompt template messages by replacing placeholders with provided values.\n\n    Examples:\n        &gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n        &gt;&gt;&gt; prompt_template = ChatPromptTemplate.load_from_hub(\n        ...     repo_id=\"MoritzLaurer/example_prompts\",\n        ...     filename=\"code_teacher.yaml\"\n        ... )\n        &gt;&gt;&gt; messages = prompt_template.populate(\n        ...     concept=\"list comprehension\",\n        ...     programming_language=\"Python\"\n        ... )\n        &gt;&gt;&gt; print(messages)\n        [{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]\n\n    Args:\n        **user_provided_variables: The values to fill placeholders in the messages template.\n\n    Returns:\n        List[Dict[str, Any]]: The populated prompt as a list in the OpenAI messages format.\n    \"\"\"\n    self._validate_user_provided_variables(user_provided_variables)\n\n    messages_template_populated: List[Dict[str, str]] = [\n        {\n            \"role\": str(message[\"role\"]),\n            \"content\": self._populate_placeholders(message[\"content\"], user_provided_variables),\n        }\n        for message in self.template\n    ]\n    return messages_template_populated\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.ChatPromptTemplate.create_messages","title":"create_messages","text":"<pre><code>create_messages(client='openai', **user_provided_variables)\n</code></pre> <p>Convenience method that populates a prompt template and formats it for a client in one step. This method is only useful if your a client that does not use the OpenAI messages format, because populating a ChatPromptTemplate converts it into the OpenAI messages format by default.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n&gt;&gt;&gt; prompt_template = ChatPromptTemplate.load_from_hub(\n...     repo_id=\"MoritzLaurer/example_prompts\",\n...     filename=\"code_teacher.yaml\"\n... )\n&gt;&gt;&gt; # Format for OpenAI (default)\n&gt;&gt;&gt; messages = prompt_template.create_messages(\n...     concept=\"list comprehension\",\n...     programming_language=\"Python\"\n... )\n&gt;&gt;&gt; print(messages)\n[{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]\n</code></pre> <pre><code>&gt;&gt;&gt; # Format for Anthropic\n&gt;&gt;&gt; messages = prompt_template.create_messages(\n...     client=\"anthropic\",\n...     concept=\"list comprehension\",\n...     programming_language=\"Python\"\n... )\n&gt;&gt;&gt; messages\n{'system': 'You are a coding assistant who explains concepts clearly and provides short examples.', 'messages': [{'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>str</code> <p>The client format to use ('openai', 'anthropic', 'google'). Defaults to 'openai'.</p> <code>'openai'</code> <code>**user_provided_variables</code> <code>Any</code> <p>The variables to fill into the prompt template. For example, if your template expects variables like 'name' and 'age', pass them as keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]] | Dict[str, Any]</code> <p>List[Dict[str, Any]] | Dict[str, Any]: A populated prompt formatted for the specified client.</p> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>def create_messages(\n    self, client: ClientType = \"openai\", **user_provided_variables: Any\n) -&gt; List[Dict[str, Any]] | Dict[str, Any]:\n    \"\"\"Convenience method that populates a prompt template and formats it for a client in one step.\n    This method is only useful if your a client that does not use the OpenAI messages format, because\n    populating a ChatPromptTemplate converts it into the OpenAI messages format by default.\n\n    Examples:\n        &gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n        &gt;&gt;&gt; prompt_template = ChatPromptTemplate.load_from_hub(\n        ...     repo_id=\"MoritzLaurer/example_prompts\",\n        ...     filename=\"code_teacher.yaml\"\n        ... )\n        &gt;&gt;&gt; # Format for OpenAI (default)\n        &gt;&gt;&gt; messages = prompt_template.create_messages(\n        ...     concept=\"list comprehension\",\n        ...     programming_language=\"Python\"\n        ... )\n        &gt;&gt;&gt; print(messages)\n        [{'role': 'system', 'content': 'You are a coding assistant who explains concepts clearly and provides short examples.'}, {'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]\n\n        &gt;&gt;&gt; # Format for Anthropic\n        &gt;&gt;&gt; messages = prompt_template.create_messages(\n        ...     client=\"anthropic\",\n        ...     concept=\"list comprehension\",\n        ...     programming_language=\"Python\"\n        ... )\n        &gt;&gt;&gt; messages\n        {'system': 'You are a coding assistant who explains concepts clearly and provides short examples.', 'messages': [{'role': 'user', 'content': 'Explain what list comprehension is in Python.'}]}\n\n    Args:\n        client (str): The client format to use ('openai', 'anthropic', 'google'). Defaults to 'openai'.\n        **user_provided_variables: The variables to fill into the prompt template. For example, if your template\n            expects variables like 'name' and 'age', pass them as keyword arguments.\n\n    Returns:\n        List[Dict[str, Any]] | Dict[str, Any]: A populated prompt formatted for the specified client.\n    \"\"\"\n    if \"client\" in user_provided_variables:\n        logger.warning(\n            f\"'client' was passed both as a parameter for the LLM inference client ('{client}') and in user_provided_variables \"\n            f\"('{user_provided_variables['client']}'). The first parameter version will be used for formatting, \"\n            \"while the second user_provided_variable version will be used in template population.\"\n        )\n\n    prompt = self.populate(**user_provided_variables)\n    return format_for_client(prompt, client)\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.ChatPromptTemplate.to_langchain_template","title":"to_langchain_template","text":"<pre><code>to_langchain_template()\n</code></pre> <p>Convert the ChatPromptTemplate to a LangChain ChatPromptTemplate.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n&gt;&gt;&gt; prompt_template = ChatPromptTemplate.load_from_hub(\n...     repo_id=\"MoritzLaurer/example_prompts\",\n...     filename=\"code_teacher.yaml\"\n... )\n&gt;&gt;&gt; lc_template = prompt_template.to_langchain_template()\n&gt;&gt;&gt; # test equivalence\n&gt;&gt;&gt; from langchain_core.prompts import ChatPromptTemplate as LC_ChatPromptTemplate\n&gt;&gt;&gt; isinstance(lc_template, LC_ChatPromptTemplate)\nTrue\n</code></pre> <p>Returns:</p> Name Type Description <code>ChatPromptTemplate</code> <code>ChatPromptTemplate</code> <p>A LangChain ChatPromptTemplate object.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If LangChain is not installed.</p> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>def to_langchain_template(self) -&gt; \"LC_ChatPromptTemplate\":\n    \"\"\"Convert the ChatPromptTemplate to a LangChain ChatPromptTemplate.\n\n    Examples:\n        &gt;&gt;&gt; from prompt_templates import ChatPromptTemplate\n        &gt;&gt;&gt; prompt_template = ChatPromptTemplate.load_from_hub(\n        ...     repo_id=\"MoritzLaurer/example_prompts\",\n        ...     filename=\"code_teacher.yaml\"\n        ... )\n        &gt;&gt;&gt; lc_template = prompt_template.to_langchain_template()\n        &gt;&gt;&gt; # test equivalence\n        &gt;&gt;&gt; from langchain_core.prompts import ChatPromptTemplate as LC_ChatPromptTemplate\n        &gt;&gt;&gt; isinstance(lc_template, LC_ChatPromptTemplate)\n        True\n\n    Returns:\n        ChatPromptTemplate: A LangChain ChatPromptTemplate object.\n\n    Raises:\n        ImportError: If LangChain is not installed.\n    \"\"\"\n    try:\n        from langchain_core.prompts import ChatPromptTemplate as LC_ChatPromptTemplate\n    except ImportError as e:\n        raise ImportError(\"LangChain is not installed. Please install it with 'pip install langchain'\") from e\n\n    # LangChain expects a list of tuples of the form (role, content)\n    messages: List[Tuple[str, str]] = [\n        (str(message[\"role\"]), str(message[\"content\"])) for message in self.template\n    ]\n    return LC_ChatPromptTemplate(\n        messages=messages,\n        input_variables=self.template_variables,\n        metadata=self.metadata,\n    )\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.PromptTemplateDictionary","title":"PromptTemplateDictionary","text":"<p>A container class that holds multiple prompt templates (TextPromptTemplate or ChatPromptTemplate), as defined under the \"template_dictionary\" key in a YAML file. This allows users to store and manage multiple interdependent templates in one place (e.g., for an agent that needs a system prompt, a planning prompt, etc.).</p> <p>Attributes:</p> Name Type Description <code>template_dictionary</code> <code>Dict[str, BasePromptTemplate]</code> <p>A dictionary of sub-prompt name -&gt; BasePromptTemplate objects.</p> <code>metadata</code> <code>Dict[str, Any]</code> <p>Optional top-level metadata about this multi-prompt configuration.</p> <code>client_parameters</code> <code>Dict[str, Any]</code> <p>Optional top-level inference parameters (e.g., temperature).</p> <code>custom_data</code> <code>Dict[str, Any]</code> <p>Arbitrary additional data relevant to the multi-template.</p> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>class PromptTemplateDictionary:\n    \"\"\"\n    A container class that holds multiple prompt templates (TextPromptTemplate or ChatPromptTemplate),\n    as defined under the \"template_dictionary\" key in a YAML file. This allows users to store and manage\n    multiple interdependent templates in one place (e.g., for an agent that needs a system prompt,\n    a planning prompt, etc.).\n\n    Attributes:\n        template_dictionary (Dict[str, BasePromptTemplate]):\n            A dictionary of sub-prompt name -&gt; BasePromptTemplate objects.\n        metadata (Dict[str, Any]):\n            Optional top-level metadata about this multi-prompt configuration.\n        client_parameters (Dict[str, Any]):\n            Optional top-level inference parameters (e.g., temperature).\n        custom_data (Dict[str, Any]):\n            Arbitrary additional data relevant to the multi-template.\n    \"\"\"\n\n    def __init__(\n        self,\n        template_dictionary: Dict[str, \"BasePromptTemplate\"],\n        metadata: Optional[Dict[str, Any]] = None,\n        client_parameters: Optional[Dict[str, Any]] = None,\n        custom_data: Optional[Dict[str, Any]] = None,\n    ):\n        self.template_dictionary = template_dictionary\n        self.metadata = metadata or {}\n        self.client_parameters = client_parameters or {}\n        self.custom_data = custom_data or {}\n\n    @classmethod\n    def from_dict(\n        cls,\n        prompt_file_dic: Dict[str, Any],\n        populator: PopulatorType = \"jinja2\",\n        jinja2_security_level: Jinja2SecurityLevel = \"standard\",\n    ) -&gt; \"PromptTemplateDictionary\":\n        \"\"\"\n        Parse the multi-template structure from a Python dict (typically loaded from a YAML file).\n\n        Each key under \"template_dictionary\" is treated as a separate prompt definition.\n        We detect whether to instantiate a ChatPromptTemplate or TextPromptTemplate based\n        on the \"template\" field (list vs. string).\n\n        Args:\n            prompt_file_dic: The parsed YAML as a Python dictionary.\n            populator: Which templating approach to use (e.g., jinja2).\n            jinja2_security_level: Jinja2 sandbox security level.\n\n        Returns:\n            PromptTemplateDictionary: An instance containing all sub-prompts.\n        \"\"\"\n        # TODO: double-check alignment with_load_template_from_dict (in terms of validation and naming)\n\n        # Validate YAML structure\n        if \"prompt\" not in prompt_file_dic:\n            raise ValueError(\n                f\"Invalid YAML structure: The top-level keys are {list(prompt_file_dic.keys())}. \"\n                \"The YAML file must contain the key 'prompt' as the top-level key.\"\n            )\n\n        prompt_data = prompt_file_dic[\"prompt\"]\n\n        # Extract fields\n        metadata = prompt_data.get(\"metadata\")\n        client_parameters = prompt_data.get(\"client_parameters\")\n        custom_data = {\n            k: v\n            for k, v in prompt_data.items()\n            if k not in [\"template_dictionary\", \"metadata\", \"client_parameters\", \"custom_data\"]\n        }\n        custom_data = {**prompt_data.get(\"custom_data\", {}), **custom_data}\n\n        template_dictionary_raw = prompt_data.get(\"template_dictionary\")\n        if template_dictionary_raw is None:\n            raise ValueError(\"The 'template_dictionary' key is missing from the input data.\")\n        if not isinstance(template_dictionary_raw, dict):\n            raise ValueError(\"The 'template_dictionary' must be a dictionary.\")\n\n        template_dictionary: Dict[str, BasePromptTemplate] = {}\n        for sub_template_name, sub_template in template_dictionary_raw.items():\n            # Each sub_template is itself a dict that must have \"template\" and optionally \"template_variables\", etc.\n            if \"template\" not in sub_template:\n                raise ValueError(\n                    f\"Entry '{sub_template_name}' must contain a 'template' key. \"\n                    f\"Found keys: {list(sub_template.keys())}\"\n                )\n\n            template_field = sub_template[\"template\"]\n            template_variables = sub_template.get(\"template_variables\")\n            sub_metadata = sub_template.get(\"metadata\")\n            sub_client_parameters = sub_template.get(\"client_parameters\")\n            sub_custom_data = sub_template.get(\"custom_data\")\n\n            # Decide whether it's a ChatPromptTemplate or TextPromptTemplate\n            if isinstance(template_field, list) and any(isinstance(item, dict) for item in template_field):\n                # Likely ChatPromptTemplate\n                template_dictionary[sub_template_name] = ChatPromptTemplate(\n                    template=template_field,\n                    template_variables=template_variables,\n                    metadata=sub_metadata,\n                    client_parameters=sub_client_parameters,\n                    custom_data=sub_custom_data,\n                    populator=populator,\n                    jinja2_security_level=jinja2_security_level,\n                )\n            elif isinstance(template_field, str):\n                # TextPromptTemplate\n                template_dictionary[sub_template_name] = TextPromptTemplate(\n                    template=template_field,\n                    template_variables=template_variables,\n                    metadata=sub_metadata,\n                    client_parameters=sub_client_parameters,\n                    custom_data=sub_custom_data,\n                    populator=populator,\n                    jinja2_security_level=jinja2_security_level,\n                )\n            else:\n                raise ValueError(\n                    f\"Invalid template type under '{sub_template_name}'. \"\n                    \"Template must be either a string for text prompts \"\n                    \"or a list of dicts for chat prompts.\"\n                )\n\n        return cls(\n            template_dictionary=template_dictionary,\n            metadata=metadata,\n            client_parameters=client_parameters,\n            custom_data=custom_data,\n        )\n\n    @classmethod\n    def load_from_local(\n        cls,\n        file_path: Union[str, Path],\n        populator: PopulatorType = \"jinja2\",\n        jinja2_security_level: Jinja2SecurityLevel = \"standard\",\n    ) -&gt; \"PromptTemplateDictionary\":\n        \"\"\"\n        Load a multi-prompt YAML file from the local filesystem, parse it,\n        and create a PromptTemplateDictionary.\n\n        Args:\n            file_path: Path to the YAML file.\n            populator: Templating approach (jinja2, double brace, etc.).\n            jinja2_security_level: Security level for Jinja2 sandbox.\n\n        Returns:\n            PromptTemplateDictionary with all sub-prompts.\n        \"\"\"\n        file_path = Path(file_path)\n        if not file_path.exists():\n            raise FileNotFoundError(f\"File not found: {file_path}\")\n\n        yaml_handler = create_yaml_handler(\"ruamel\")\n        with file_path.open(\"r\", encoding=\"utf-8\") as f:\n            data = yaml_handler.load(f)\n\n        return cls.from_dict(data, populator, jinja2_security_level)\n\n    def __getitem__(self, sub_template_name: str) -&gt; \"BasePromptTemplate\":\n        \"\"\"\n        Retrieve a sub-prompt by name.\n\n        Example:\n            &gt;&gt;&gt; multi_template = PromptTemplateDictionary.load_from_local(\"agent_example_1.yaml\")\n            &gt;&gt;&gt; system_prompt = multi_template[\"agent_system_prompt\"]\n            &gt;&gt;&gt; populated = system_prompt.populate(tool_descriptions=\"...\", task=\"...\")\n        \"\"\"\n        return self.template_dictionary[sub_template_name]\n\n    def populate(\n        self,\n        sub_template_name: str,\n        **user_provided_variables: Any,\n    ) -&gt; Union[str, List[Dict[str, Any]]]:\n        \"\"\"\n        Shortcut method to populate a single sub-prompt from this dictionary.\n\n        Args:\n            sub_template_name (str): The name of the sub-prompt to populate.\n            **user_provided_variables: Values for placeholders in the template.\n\n        Returns:\n            The populated prompt, either a list of message dicts (for chat)\n            or a single string (for text).\n        \"\"\"\n        if sub_template_name not in self.template_dictionary:\n            raise KeyError(f\"No sub-prompt named '{sub_template_name}' found.\")\n        return self.template_dictionary[sub_template_name].populate(**user_provided_variables)\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.PromptTemplateDictionary.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(prompt_file_dic, populator='jinja2', jinja2_security_level='standard')\n</code></pre> <p>Parse the multi-template structure from a Python dict (typically loaded from a YAML file).</p> <p>Each key under \"template_dictionary\" is treated as a separate prompt definition. We detect whether to instantiate a ChatPromptTemplate or TextPromptTemplate based on the \"template\" field (list vs. string).</p> <p>Parameters:</p> Name Type Description Default <code>prompt_file_dic</code> <code>Dict[str, Any]</code> <p>The parsed YAML as a Python dictionary.</p> required <code>populator</code> <code>PopulatorType</code> <p>Which templating approach to use (e.g., jinja2).</p> <code>'jinja2'</code> <code>jinja2_security_level</code> <code>Jinja2SecurityLevel</code> <p>Jinja2 sandbox security level.</p> <code>'standard'</code> <p>Returns:</p> Name Type Description <code>PromptTemplateDictionary</code> <code>PromptTemplateDictionary</code> <p>An instance containing all sub-prompts.</p> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>@classmethod\ndef from_dict(\n    cls,\n    prompt_file_dic: Dict[str, Any],\n    populator: PopulatorType = \"jinja2\",\n    jinja2_security_level: Jinja2SecurityLevel = \"standard\",\n) -&gt; \"PromptTemplateDictionary\":\n    \"\"\"\n    Parse the multi-template structure from a Python dict (typically loaded from a YAML file).\n\n    Each key under \"template_dictionary\" is treated as a separate prompt definition.\n    We detect whether to instantiate a ChatPromptTemplate or TextPromptTemplate based\n    on the \"template\" field (list vs. string).\n\n    Args:\n        prompt_file_dic: The parsed YAML as a Python dictionary.\n        populator: Which templating approach to use (e.g., jinja2).\n        jinja2_security_level: Jinja2 sandbox security level.\n\n    Returns:\n        PromptTemplateDictionary: An instance containing all sub-prompts.\n    \"\"\"\n    # TODO: double-check alignment with_load_template_from_dict (in terms of validation and naming)\n\n    # Validate YAML structure\n    if \"prompt\" not in prompt_file_dic:\n        raise ValueError(\n            f\"Invalid YAML structure: The top-level keys are {list(prompt_file_dic.keys())}. \"\n            \"The YAML file must contain the key 'prompt' as the top-level key.\"\n        )\n\n    prompt_data = prompt_file_dic[\"prompt\"]\n\n    # Extract fields\n    metadata = prompt_data.get(\"metadata\")\n    client_parameters = prompt_data.get(\"client_parameters\")\n    custom_data = {\n        k: v\n        for k, v in prompt_data.items()\n        if k not in [\"template_dictionary\", \"metadata\", \"client_parameters\", \"custom_data\"]\n    }\n    custom_data = {**prompt_data.get(\"custom_data\", {}), **custom_data}\n\n    template_dictionary_raw = prompt_data.get(\"template_dictionary\")\n    if template_dictionary_raw is None:\n        raise ValueError(\"The 'template_dictionary' key is missing from the input data.\")\n    if not isinstance(template_dictionary_raw, dict):\n        raise ValueError(\"The 'template_dictionary' must be a dictionary.\")\n\n    template_dictionary: Dict[str, BasePromptTemplate] = {}\n    for sub_template_name, sub_template in template_dictionary_raw.items():\n        # Each sub_template is itself a dict that must have \"template\" and optionally \"template_variables\", etc.\n        if \"template\" not in sub_template:\n            raise ValueError(\n                f\"Entry '{sub_template_name}' must contain a 'template' key. \"\n                f\"Found keys: {list(sub_template.keys())}\"\n            )\n\n        template_field = sub_template[\"template\"]\n        template_variables = sub_template.get(\"template_variables\")\n        sub_metadata = sub_template.get(\"metadata\")\n        sub_client_parameters = sub_template.get(\"client_parameters\")\n        sub_custom_data = sub_template.get(\"custom_data\")\n\n        # Decide whether it's a ChatPromptTemplate or TextPromptTemplate\n        if isinstance(template_field, list) and any(isinstance(item, dict) for item in template_field):\n            # Likely ChatPromptTemplate\n            template_dictionary[sub_template_name] = ChatPromptTemplate(\n                template=template_field,\n                template_variables=template_variables,\n                metadata=sub_metadata,\n                client_parameters=sub_client_parameters,\n                custom_data=sub_custom_data,\n                populator=populator,\n                jinja2_security_level=jinja2_security_level,\n            )\n        elif isinstance(template_field, str):\n            # TextPromptTemplate\n            template_dictionary[sub_template_name] = TextPromptTemplate(\n                template=template_field,\n                template_variables=template_variables,\n                metadata=sub_metadata,\n                client_parameters=sub_client_parameters,\n                custom_data=sub_custom_data,\n                populator=populator,\n                jinja2_security_level=jinja2_security_level,\n            )\n        else:\n            raise ValueError(\n                f\"Invalid template type under '{sub_template_name}'. \"\n                \"Template must be either a string for text prompts \"\n                \"or a list of dicts for chat prompts.\"\n            )\n\n    return cls(\n        template_dictionary=template_dictionary,\n        metadata=metadata,\n        client_parameters=client_parameters,\n        custom_data=custom_data,\n    )\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.PromptTemplateDictionary.load_from_local","title":"load_from_local  <code>classmethod</code>","text":"<pre><code>load_from_local(file_path, populator='jinja2', jinja2_security_level='standard')\n</code></pre> <p>Load a multi-prompt YAML file from the local filesystem, parse it, and create a PromptTemplateDictionary.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[str, Path]</code> <p>Path to the YAML file.</p> required <code>populator</code> <code>PopulatorType</code> <p>Templating approach (jinja2, double brace, etc.).</p> <code>'jinja2'</code> <code>jinja2_security_level</code> <code>Jinja2SecurityLevel</code> <p>Security level for Jinja2 sandbox.</p> <code>'standard'</code> <p>Returns:</p> Type Description <code>PromptTemplateDictionary</code> <p>PromptTemplateDictionary with all sub-prompts.</p> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>@classmethod\ndef load_from_local(\n    cls,\n    file_path: Union[str, Path],\n    populator: PopulatorType = \"jinja2\",\n    jinja2_security_level: Jinja2SecurityLevel = \"standard\",\n) -&gt; \"PromptTemplateDictionary\":\n    \"\"\"\n    Load a multi-prompt YAML file from the local filesystem, parse it,\n    and create a PromptTemplateDictionary.\n\n    Args:\n        file_path: Path to the YAML file.\n        populator: Templating approach (jinja2, double brace, etc.).\n        jinja2_security_level: Security level for Jinja2 sandbox.\n\n    Returns:\n        PromptTemplateDictionary with all sub-prompts.\n    \"\"\"\n    file_path = Path(file_path)\n    if not file_path.exists():\n        raise FileNotFoundError(f\"File not found: {file_path}\")\n\n    yaml_handler = create_yaml_handler(\"ruamel\")\n    with file_path.open(\"r\", encoding=\"utf-8\") as f:\n        data = yaml_handler.load(f)\n\n    return cls.from_dict(data, populator, jinja2_security_level)\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.PromptTemplateDictionary.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(sub_template_name)\n</code></pre> <p>Retrieve a sub-prompt by name.</p> Example <p>multi_template = PromptTemplateDictionary.load_from_local(\"agent_example_1.yaml\") system_prompt = multi_template[\"agent_system_prompt\"] populated = system_prompt.populate(tool_descriptions=\"...\", task=\"...\")</p> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>def __getitem__(self, sub_template_name: str) -&gt; \"BasePromptTemplate\":\n    \"\"\"\n    Retrieve a sub-prompt by name.\n\n    Example:\n        &gt;&gt;&gt; multi_template = PromptTemplateDictionary.load_from_local(\"agent_example_1.yaml\")\n        &gt;&gt;&gt; system_prompt = multi_template[\"agent_system_prompt\"]\n        &gt;&gt;&gt; populated = system_prompt.populate(tool_descriptions=\"...\", task=\"...\")\n    \"\"\"\n    return self.template_dictionary[sub_template_name]\n</code></pre>"},{"location":"reference/prompt_templates/#prompt_templates.prompt_templates.PromptTemplateDictionary.populate","title":"populate","text":"<pre><code>populate(sub_template_name, **user_provided_variables)\n</code></pre> <p>Shortcut method to populate a single sub-prompt from this dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>sub_template_name</code> <code>str</code> <p>The name of the sub-prompt to populate.</p> required <code>**user_provided_variables</code> <code>Any</code> <p>Values for placeholders in the template.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, List[Dict[str, Any]]]</code> <p>The populated prompt, either a list of message dicts (for chat)</p> <code>Union[str, List[Dict[str, Any]]]</code> <p>or a single string (for text).</p> Source code in <code>prompt_templates/prompt_templates.py</code> <pre><code>def populate(\n    self,\n    sub_template_name: str,\n    **user_provided_variables: Any,\n) -&gt; Union[str, List[Dict[str, Any]]]:\n    \"\"\"\n    Shortcut method to populate a single sub-prompt from this dictionary.\n\n    Args:\n        sub_template_name (str): The name of the sub-prompt to populate.\n        **user_provided_variables: Values for placeholders in the template.\n\n    Returns:\n        The populated prompt, either a list of message dicts (for chat)\n        or a single string (for text).\n    \"\"\"\n    if sub_template_name not in self.template_dictionary:\n        raise KeyError(f\"No sub-prompt named '{sub_template_name}' found.\")\n    return self.template_dictionary[sub_template_name].populate(**user_provided_variables)\n</code></pre>"},{"location":"reference/utils/","title":"Utility functions","text":"<p>This section documents utility functions.</p>"},{"location":"reference/utils/#prompt_templates.utils","title":"prompt_templates.utils","text":""},{"location":"reference/utils/#prompt_templates.utils.list_prompt_templates","title":"list_prompt_templates","text":"<pre><code>list_prompt_templates(repo_id, repo_type='dataset', token=None)\n</code></pre> <p>List available prompt template YAML files in a Hugging Face Hub repository.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>The repository ID on Hugging Face Hub.</p> required <code>repo_type</code> <code>Optional[str]</code> <p>The type of repository. Defaults to \"dataset\".</p> <code>'dataset'</code> <code>token</code> <code>Optional[str]</code> <p>An optional authentication token. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of YAML filenames in the repository sorted alphabetically.</p> <p>Examples:</p> <p>List all prompt templates in a repository:</p> <pre><code>&gt;&gt;&gt; from prompt_templates import list_prompt_templates\n&gt;&gt;&gt; files = list_prompt_templates(\"MoritzLaurer/example_prompts\")\n&gt;&gt;&gt; files\n['code_teacher.yaml', 'translate.yaml', 'translate_jinja2.yaml']\n</code></pre> Note <p>This function simply returns all YAML file names in the repository. It does not validate if the files contain valid prompt templates, which would require downloading them.</p> Source code in <code>prompt_templates/utils.py</code> <pre><code>def list_prompt_templates(\n    repo_id: str, repo_type: Optional[str] = \"dataset\", token: Optional[str] = None\n) -&gt; List[str]:\n    \"\"\"List available prompt template YAML files in a Hugging Face Hub repository.\n\n    Args:\n        repo_id (str): The repository ID on Hugging Face Hub.\n        repo_type (Optional[str]): The type of repository. Defaults to \"dataset\".\n        token (Optional[str]): An optional authentication token. Defaults to None.\n\n    Returns:\n        List[str]: A list of YAML filenames in the repository sorted alphabetically.\n\n    Examples:\n        List all prompt templates in a repository:\n        &gt;&gt;&gt; from prompt_templates import list_prompt_templates\n        &gt;&gt;&gt; files = list_prompt_templates(\"MoritzLaurer/example_prompts\")\n        &gt;&gt;&gt; files\n        ['code_teacher.yaml', 'translate.yaml', 'translate_jinja2.yaml']\n\n    Note:\n        This function simply returns all YAML file names in the repository.\n        It does not validate if the files contain valid prompt templates, which would require downloading them.\n    \"\"\"\n    logger.info(\n        \"This function simply returns all YAML file names in the repository. \"\n        \"It does not validate if the files contain valid prompt templates, which would require downloading them.\"\n    )\n    api = HfApi(token=token)\n    yaml_files = [\n        file for file in api.list_repo_files(repo_id, repo_type=repo_type) if file.endswith(VALID_PROMPT_EXTENSIONS)\n    ]\n    return sorted(yaml_files)\n</code></pre>"},{"location":"reference/utils/#prompt_templates.utils.format_for_client","title":"format_for_client","text":"<pre><code>format_for_client(messages, client='openai')\n</code></pre> <p>Format OpenAI-style chat messages for different LLM clients.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Dict[str, Any]]</code> <p>List of message dictionaries in OpenAI format</p> required <code>client</code> <code>ClientType</code> <p>The client format to use ('openai', 'anthropic', 'google'). Defaults to 'openai'</p> <code>'openai'</code> <p>Returns:</p> Type Description <code>Union[List[Dict[str, Any]], Dict[str, Any]]</code> <p>Messages formatted for the specified client</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported client format is specified</p> <code>TypeError</code> <p>If messages is not a list of dicts</p> <p>Examples:</p> <p>Format messages for different LLM clients:</p> <pre><code>&gt;&gt;&gt; messages = [\n...     {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n...     {\"role\": \"user\", \"content\": \"Hello!\"}\n... ]\n&gt;&gt;&gt; # OpenAI format (default, no change)\n&gt;&gt;&gt; openai_messages = format_for_client(messages)\n&gt;&gt;&gt; print(openai_messages)\n[{'role': 'system', 'content': 'You are a helpful assistant'}, {'role': 'user', 'content': 'Hello!'}]\n</code></pre> <pre><code>&gt;&gt;&gt; # Anthropic format\n&gt;&gt;&gt; anthropic_messages = format_for_client(messages, \"anthropic\")\n&gt;&gt;&gt; print(anthropic_messages)\n{'system': 'You are a helpful assistant', 'messages': [{'role': 'user', 'content': 'Hello!'}]}\n</code></pre> <pre><code>&gt;&gt;&gt; # Google (Gemini) format\n&gt;&gt;&gt; google_messages = format_for_client(messages, \"google\")\n&gt;&gt;&gt; print(google_messages)\n{'system_instruction': 'You are a helpful assistant', 'contents': 'Hello!'}\n</code></pre> Source code in <code>prompt_templates/utils.py</code> <pre><code>def format_for_client(\n    messages: List[Dict[str, Any]], client: ClientType = \"openai\"\n) -&gt; Union[List[Dict[str, Any]], Dict[str, Any]]:\n    \"\"\"Format OpenAI-style chat messages for different LLM clients.\n\n    Args:\n        messages: List of message dictionaries in OpenAI format\n        client: The client format to use ('openai', 'anthropic', 'google'). Defaults to 'openai'\n\n    Returns:\n        Messages formatted for the specified client\n\n    Raises:\n        ValueError: If an unsupported client format is specified\n        TypeError: If messages is not a list of dicts\n\n    Examples:\n        Format messages for different LLM clients:\n        &gt;&gt;&gt; messages = [\n        ...     {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n        ...     {\"role\": \"user\", \"content\": \"Hello!\"}\n        ... ]\n        &gt;&gt;&gt; # OpenAI format (default, no change)\n        &gt;&gt;&gt; openai_messages = format_for_client(messages)\n        &gt;&gt;&gt; print(openai_messages)\n        [{'role': 'system', 'content': 'You are a helpful assistant'}, {'role': 'user', 'content': 'Hello!'}]\n\n        &gt;&gt;&gt; # Anthropic format\n        &gt;&gt;&gt; anthropic_messages = format_for_client(messages, \"anthropic\")\n        &gt;&gt;&gt; print(anthropic_messages)\n        {'system': 'You are a helpful assistant', 'messages': [{'role': 'user', 'content': 'Hello!'}]}\n\n        &gt;&gt;&gt; # Google (Gemini) format\n        &gt;&gt;&gt; google_messages = format_for_client(messages, \"google\")\n        &gt;&gt;&gt; print(google_messages)\n        {'system_instruction': 'You are a helpful assistant', 'contents': 'Hello!'}\n    \"\"\"\n    if not isinstance(messages, list) or not all(isinstance(msg, dict) for msg in messages):\n        raise TypeError(\"Messages must be a list of dictionaries\")\n\n    if client == \"openai\":\n        return messages\n    elif client == \"anthropic\":\n        return format_for_anthropic(messages)\n    elif client == \"google\":\n        return format_for_google(messages)\n    else:\n        raise ValueError(f\"Unsupported client format: {client}. Supported formats are: {list(get_args(ClientType))}\")\n</code></pre>"},{"location":"reference/utils/#prompt_templates.utils.format_for_anthropic","title":"format_for_anthropic","text":"<pre><code>format_for_anthropic(messages)\n</code></pre> <p>Format OpenAI-style messages for the Anthropic client.</p> <p>Converts OpenAI-style messages to Anthropic's expected format by: 1. Extracting the system message (if any) into a top-level 'system' key 2. Moving all non-system messages into a 'messages' list</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Dict[str, Any]]</code> <p>List of message dictionaries in OpenAI format</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict with 'system' and 'messages' keys formatted for Anthropic</p> Source code in <code>prompt_templates/utils.py</code> <pre><code>def format_for_anthropic(messages: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n    \"\"\"Format OpenAI-style messages for the Anthropic client.\n\n    Converts OpenAI-style messages to Anthropic's expected format by:\n    1. Extracting the system message (if any) into a top-level 'system' key\n    2. Moving all non-system messages into a 'messages' list\n\n    Args:\n        messages: List of message dictionaries in OpenAI format\n\n    Returns:\n        Dict with 'system' and 'messages' keys formatted for Anthropic\n    \"\"\"\n    return {\n        \"system\": next((msg[\"content\"] for msg in messages if msg[\"role\"] == \"system\"), None),\n        \"messages\": [msg for msg in messages if msg[\"role\"] != \"system\"],\n    }\n</code></pre>"},{"location":"reference/utils/#prompt_templates.utils.format_for_google","title":"format_for_google","text":"<pre><code>format_for_google(messages)\n</code></pre> <p>Format OpenAI-style messages for the Google GenAI SDK's generate_content method.</p> <p>Converts OpenAI-style messages to Google Gemini's expected format by: 1. Extracting the system message (if any) into a top-level 'system_instruction' key 2. Moving all non-system messages into a 'contents' list of messages as <code>Part</code> objects (or a single string if there's only one message).</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Dict[str, Any]]</code> <p>List of message dictionaries in OpenAI format</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict with 'system_instruction' and 'contents' keys formatted for Google Gemini</p> Source code in <code>prompt_templates/utils.py</code> <pre><code>def format_for_google(messages: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n    \"\"\"Format OpenAI-style messages for the Google GenAI SDK's generate_content method.\n\n    Converts OpenAI-style messages to Google Gemini's expected format by:\n    1. Extracting the system message (if any) into a top-level 'system_instruction' key\n    2. Moving all non-system messages into a 'contents' list of messages as `Part` objects\n    (or a single string if there's only one message).\n\n    Args:\n        messages: List of message dictionaries in OpenAI format\n\n    Returns:\n        Dict with 'system_instruction' and 'contents' keys formatted for Google Gemini\n    \"\"\"\n    from google.genai import types\n\n    system_instruction: Optional[str] = None\n    contents: List[types.Content] = []\n\n    for msg in messages:\n        if msg[\"role\"] == \"system\":\n            system_instruction = msg[\"content\"]\n        elif msg[\"role\"] == \"user\":\n            contents.append(types.Content(parts=[types.Part.from_text(msg[\"content\"])], role=\"user\"))\n        elif msg[\"role\"] == \"assistant\":\n            contents.append(types.Content(parts=[types.Part.from_text(msg[\"content\"])], role=\"model\"))\n        else:\n            raise ValueError(f\"Unsupported role: {msg['role']}\")\n\n    # If there's only one message, simplify to just the text content\n    if len(contents) == 1:\n        contents = contents[0].parts[0].text\n\n    return {\n        \"system_instruction\": system_instruction,\n        \"contents\": contents,\n    }\n</code></pre>"},{"location":"reference/utils/#prompt_templates.utils.create_yaml_handler","title":"create_yaml_handler","text":"<pre><code>create_yaml_handler(library='ruamel')\n</code></pre> <p>Create a YAML handler with the specified configuration. Ruamel is the default, because it allows for better format preservation and defaults to the newer YAML 1.2. Pyyaml can also be used, as it can be faster and is more widely used.</p> <p>Parameters:</p> Name Type Description Default <code>library</code> <code>str</code> <p>The YAML library to use (\"ruamel\" or \"pyyaml\"). Defaults to \"ruamel\".</p> <code>'ruamel'</code> <p>Returns:</p> Type Description <code>Union[YAML, Any]</code> <p>A configured YAML handler</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported YAML library is specified</p> Source code in <code>prompt_templates/utils.py</code> <pre><code>def create_yaml_handler(library: str = \"ruamel\") -&gt; Union[YAML, Any]:\n    \"\"\"Create a YAML handler with the specified configuration.\n    Ruamel is the default, because it allows for better format preservation and defaults to the newer YAML 1.2.\n    Pyyaml can also be used, as it can be faster and is more widely used.\n\n    Args:\n        library: The YAML library to use (\"ruamel\" or \"pyyaml\"). Defaults to \"ruamel\".\n\n    Returns:\n        A configured YAML handler\n\n    Raises:\n        ValueError: If an unsupported YAML library is specified\n    \"\"\"\n    if library == \"ruamel\":\n        yaml = YAML(typ=\"rt\")\n        yaml.preserve_quotes = True\n        yaml.default_flow_style = False\n        yaml.width = 120\n        yaml.indent(mapping=2, sequence=4, offset=2)\n        return yaml\n    elif library == \"pyyaml\":\n        return pyyaml\n    else:\n        raise ValueError(f\"Unsupported YAML library: {library}\")\n</code></pre>"},{"location":"reference/utils/#prompt_templates.utils.format_template_content","title":"format_template_content","text":"<pre><code>format_template_content(node)\n</code></pre> <p>Recursively format content strings to use YAML literal block scalars. This is used to make the string outputs in a yaml file contain \"|-\", which makes the string behave like a \"\"\"...\"\"\" block in python to make strings easier to read and edit.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Any</code> <p>The prompt template content to format</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The formatted content with literal block scalars for multiline strings</p> Source code in <code>prompt_templates/utils.py</code> <pre><code>def format_template_content(node: Any) -&gt; Any:\n    '''Recursively format content strings to use YAML literal block scalars.\n    This is used to make the string outputs in a yaml file contain \"|-\",\n    which makes the string behave like a \"\"\"...\"\"\" block in python\n    to make strings easier to read and edit.\n\n    Args:\n        node: The prompt template content to format\n\n    Returns:\n        The formatted content with literal block scalars for multiline strings\n    '''\n    if isinstance(node, dict):\n        for key, value in node.items():\n            node[key] = LiteralScalarString(value.strip()) if key in [\"content\", \"text\"] else value\n        return node\n    elif isinstance(node, str):\n        if \"\\n\" in node or len(node) &gt; 80:\n            return LiteralScalarString(node.strip())\n        else:\n            return node\n    else:\n        return node\n</code></pre>"}]}